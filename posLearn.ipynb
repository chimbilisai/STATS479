{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"posLearn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"k-DyPbL26IJK","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","import numpy as np\n","import torch\n","import pandas as pd\n","import csv\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","# conditional memory\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dNO5Lf5D-npZ","colab_type":"code","outputId":"b9800a1e-5485-4df4-81ea-78f3af38d356","executionInfo":{"status":"ok","timestamp":1556511282913,"user_tz":300,"elapsed":2887,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","#sys.path.append('/content/drive/My Drive/Collij/STAT 479 Machine Learning/Deep Learning Project/DEEPakChopra/')\n","sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n","#sys.path.append('/content/drive/My Drive/')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"PQW1Lghz-szr","colab_type":"code","outputId":"b9605b7e-cf2a-4847-dc89-0fd9dff479be","executionInfo":{"status":"ok","timestamp":1556511288773,"user_tz":300,"elapsed":422,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["########## BEGIN CharRNN ##########\n","# Character mapping to integers\n","#with open(\"/content/drive/My Drive/Collij/STAT 479 Machine Learning/Deep Learning Project/DEEPakChopra/cleanedTweets2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","\n","#with open(\"/content/drive/My Drive/cleanedTweets2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","#with open(\"/content/drive/My Drive/Colab Notebooks/cleanedTweets2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","#with open(\"/content/drive/My Drive/Colab Notebooks/output.csv\", 'r', encoding='utf-8') as f:data=f.read()\n","\n","df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/output1.csv\")  \n","\n","#below was once tweetChars\n","#pos = ['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']# uniques in tweet chars\n","#print(df['data'])\n","#print(data)\n","data = df['data'].tolist()\n","pos = []\n","pos = set(data)\n","pos2int = {ch:i for i,ch in enumerate(pos)}\n","int2pos = dict(enumerate(pos))\n","pos_ints = np.array([pos2int[w] for w in data],dtype=np.int32)\n","print(pos)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["{',', ')', 'WP', 'VBZ', 'CC', 'RBS', 'JJS', 'RB', 'VB', 'RP', 'CD', 'IN', 'PRP$', 'NN', 'TO', 'WDT', ':', 'EX', 'NNS', '.', 'VBN', 'WP$', 'VBP', 'VBD', 'NNP', '(', 'UH', 'WRB', 'JJR', 'RBR', 'JJ', 'PDT', 'NNPS', 'DT', 'FW', \"''\", 'VBG', 'PRP', 'MD'}\n"],"name":"stdout"}]},{"metadata":{"id":"_zltd2SF5IyM","colab_type":"code","colab":{}},"cell_type":"code","source":["# hyperparameters\n","\n","_lstm_size=150\n","_num_layers=2\n","_learning_rate=0.001\n","_keep_prob=0.90\n","_grad_clip=5\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tyq919Mg9WDY","colab_type":"code","colab":{}},"cell_type":"code","source":["batches = []\n","\n","# Function for splitting data\n","def split_data(sequence, batch_size, num_steps):\n","    total_length = batch_size * num_steps\n","    num_batches = int(len(sequence) / total_length)\n","    if num_batches*total_length + 1 > len(sequence):\n","        num_batches = num_batches - 1\n","    # Cut down character stream to length of a batch\n","    inputs = sequence[0: num_batches * total_length]\n","    output = sequence[1: num_batches * total_length + 1]\n","    # Split input & output:\n","    split_input = np.split(inputs, batch_size)\n","    split_output = np.split(output, batch_size)\n","    # Combine the batches\n","    inputs = np.stack(split_input)\n","    output = np.stack(split_output)\n","    return inputs, output\n","\n","\n","def create_batch_generator(data_x, data_y, num_steps):\n","    batch_size, total_length = data_x.shape\n","    num_batches = int(total_length/num_steps)\n","    for b in range(num_batches):\n","        yield (data_x[:, b*num_steps:(b+1)*num_steps],\n","               data_y[:, b*num_steps:(b+1)*num_steps])\n","\n","\n","def get_top_char(probas, char_size, top_n=5):\n","    p = np.squeeze(probas)\n","    p[np.argsort(p)[:-top_n]] = 0.0\n","    p = p / np.sum(p)\n","    ch_id = np.random.choice(char_size, 1, p=p)[0]\n","    return ch_id\n","\n","# Class for character level recurrent neural network\n","class CharRNN(object):\n","    # Constructor (note, the sampling parameter is for determining\n","    # what mode this object is in (training/sampling), and grad_clip\n","    # is for preventing exploding gradients\n","    def __init__(self, num_classes, batch_size=64,\n","                 num_steps=100, lstm_size = _lstm_size, #lstm_size=128,\n","                 num_layers = _num_layers, learning_rate = _learning_rate, #num_layers=1, learning_rate=0.001,\n","                 keep_prob = _keep_prob, grad_clip = _grad_clip, #keep_prob=0.5, grad_clip=5,\n","                 sampling=False):\n","        # Set variables to values given by parameters\n","        self.num_classes = num_classes\n","        self.batch_size = batch_size\n","        self.num_steps = num_steps\n","        self.lstm_size = lstm_size\n","        self.num_layers = num_layers\n","        self.learning_rate = learning_rate\n","        self.keep_prob = keep_prob\n","        self.grad_clip = grad_clip\n","        self.g = tf.Graph()\n","        self.saver = None\n","        with self.g.as_default():\n","            tf.set_random_seed(123)\n","            self.build(sampling=sampling)  # builds x and y graphs of data\n","            self.init_op = tf.global_variables_initializer()\n","            \n","            \n","\n","    def build(self, sampling):\n","        if sampling:\n","            batch_size, num_steps = 1, 1\n","        else:\n","            batch_size = self.batch_size\n","            num_steps = self.num_steps\n","        a = tf.contrib.rnn.BasicLSTMCell(self.lstm_size)\n","        self.saver = tf.train.Saver()\n","        \n","        tf_x = tf.placeholder(tf.int32,\n","                              shape=[batch_size, num_steps],\n","                              name='tf_x')\n","        tf_y = tf.placeholder(tf.int32,\n","                              shape=[batch_size, num_steps],\n","                              name='tf_y')\n","        tf_keepprob = tf.placeholder(tf.float32,\n","                                     name='tf_keepprob')\n","\n","        # One-hot encoding:\n","        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n","        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n","\n","        \n","        # Build the multi-layer RNN cells\n","        #cells = tf.contrib.rnn.MultiRNNCell(\n","        #    [tf.contrib.rnn.DropoutWrapper(\n","        #        tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n","        #        output_keep_prob=tf_keepprob)\n","        #    for _ in range(self.num_layers)])\n","\n","        cells = tf.contrib.rnn.MultiRNNCell(\n","            [tf.contrib.rnn.DropoutWrapper(\n","                a,\n","                output_keep_prob=tf_keepprob)\n","            for _ in range(self.num_layers)])\n","        \n","        \n","        # Define the initial state\n","        self.initial_state = cells.zero_state(\n","            batch_size, tf.float32)\n","\n","        # Run each sequence step through the RNN\n","        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n","            cells, x_onehot,\n","            initial_state=self.initial_state)\n","        print('<< lstm_outputs >>', lstm_outputs)\n","        seq_output_reshaped = tf.reshape(\n","            lstm_outputs,\n","            shape=[-1, self.lstm_size],\n","            name='seq_output_reshaped')\n","\n","        logits = tf.layers.dense(\n","            inputs=seq_output_reshaped,\n","            units=self.num_classes,\n","            activation=None,\n","            name='logits')\n","        probas = tf.nn.softmax(\n","            logits,\n","            name='probabilities')\n","\n","        y_reshaped = tf.reshape(\n","            y_onehot,\n","            shape=[-1, self.num_classes],\n","            name='y_reshaped')\n","        cost = tf.reduce_mean(\n","            tf.nn.softmax_cross_entropy_with_logits(\n","                logits=logits,\n","                labels=y_reshaped),\n","            name='cost')\n","\n","        # Gradient clipping to avoid \"exploding gradients\"\n","        tvars = tf.trainable_variables()\n","        grads, _ = tf.clip_by_global_norm(\n","            tf.gradients(cost, tvars),\n","            self.grad_clip)\n","        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n","        \n","        \n","        train_op = optimizer.apply_gradients(\n","            zip(grads, tvars),\n","            name='train_op')\n","\n","\n","    def train(self, train_x, train_y,\n","        num_epochs, ckpt_dir='./model/'):\n","\n","        # Create the checkpoint directory\n","        # if it does not exists\n","        if not os.path.exists(ckpt_dir):\n","            os.mkdir(ckpt_dir)\n","\n","        with tf.Session(graph=self.g) as sess:\n","            sess.run(self.init_op)\n","            n_batches = int(train_x.shape[1] / self.num_steps)\n","            iterations = n_batches * num_epochs\n","            for epoch in range(num_epochs):\n","\n","                # Train network\n","                new_state = sess.run(self.initial_state)\n","                loss = 0\n","\n","                # Mini-batch generator\n","                minibatchgen = create_batch_generator(\n","                    train_x, train_y, self.num_steps)\n","\n","                for b, (batch_x, batch_y) in enumerate(minibatchgen, 1):\n","                    iteration = epoch * n_batches + b\n","                    feed = {'tf_x:0': batch_x,\n","                            'tf_y:0': batch_y,\n","                            'tf_keepprob:0': self.keep_prob,\n","                            self.initial_state: new_state}\n","                    batch_cost, _, new_state = sess.run(\n","                        ['cost:0', 'train_op',\n","                         self.final_state],\n","                        feed_dict=feed)\n","                    if iteration % 10 == 0:\n","                        print('Epoch %d/%d Iteration %d'\n","                              '| Training loss: %.4f' % (\n","                                  epoch + 1, num_epochs,\n","                                  iteration, batch_cost))\n","                        batches.append(batch_cost)\n","\n","                # Save the trained model\n","                self.saver.save(\n","                    sess, os.path.join(\n","                        ckpt_dir, 'language_modeling.ckpt'))\n","\n","\n","    def sample(self, output_length,\n","               ckpt_dir, starter_seq=\".\"):\n","        #observed_seq = [ch for ch in starter_seq]\n","        observed_seq = [starter_seq]\n","        with tf.Session(graph=self.g) as sess:\n","            self.saver.restore(\n","            sess,\n","            tf.train.latest_checkpoint(ckpt_dir))\n","\n","            # 1: run the model using the starter sequence\n","            new_state = sess.run(self.initial_state)\n","            for ch in starter_seq:\n","                x = np.zeros((1, 1))\n","                x[0, 0] = pos2int[ch.lower()]\n","                feed = {'tf_x:0': x,\n","                        'tf_keepprob:0': 1.0,\n","                        self.initial_state: new_state}\n","                probas, new_state = sess.run(\n","                            ['probabilities:0', self.final_state],\n","                            feed_dict=feed)\n","            ch_id = get_top_char(probas, len(pos))\n","            observed_seq.append(int2pos[ch_id])\n","\n","            # 2: run the model using the updated observed_seq\n","            for i in range(output_length):\n","                x[0,0] = ch_id\n","                feed = {'tf_x:0': x,\n","                'tf_keepprob:0': 1.0,\n","                        self.initial_state: new_state}\n","                probas, new_state = sess.run(\n","                                ['probabilities:0', self.final_state],\n","                                feed_dict=feed)\n","\n","                ch_id = get_top_char(probas, len(pos))\n","                observed_seq.append(int2pos[ch_id])\n","        outtie = \"\"\n","        for i in range(len(observed_seq)):\n","          outtie = outtie + observed_seq[i] + \" \"\n","        print(outtie)\n","        return observed_seq"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RGk8cdqXH1DM","colab_type":"code","outputId":"e86e324e-7c5a-4a3c-c250-d5492b2ad230","executionInfo":{"status":"error","timestamp":1556511844561,"user_tz":300,"elapsed":612,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":395}},"cell_type":"code","source":["# Begin executing CharRNN\n","batch_size = 64\n","num_steps = 100\n","train_x, train_y = split_data(pos_ints,\n","                                batch_size,\n","                                num_steps)\n","rnn = CharRNN(num_classes=len(pos), batch_size=batch_size)\n","rnn.train(train_x, train_y,\n","          num_epochs=60,\n","          ckpt_dir='/content/drive/My Drive/posmodel-100/')\n","\n","del rnn\n","np.random.seed(123)\n","rnn = CharRNN(len(pos), sampling=True)\n","#print(rnn.sample(ckpt_dir='/content/drive/My Drive/Colab Notebooks/model-100/', output_length=35))\n","obs_seq = rnn.sample(ckpt_dir='/content/drive/My Drive/posmodel-100/', output_length=45)\n","plt.plot(batches)\n","plt.show()"],"execution_count":14,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-2f61e56fcef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 num_steps)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m rnn.train(train_x, train_y,\n\u001b[1;32m      8\u001b[0m           \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-cbb7cecab9f0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, keep_prob, grad_clip, sampling)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# builds x and y graphs of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-cbb7cecab9f0>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, sampling)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         tf_x = tf.placeholder(tf.int32,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    830\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    831\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    867\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No variables to save"]}]},{"metadata":{"id":"os8sFN1kI6OS","colab_type":"code","outputId":"42289352-a652-481d-9259-7fcd9b0632ce","executionInfo":{"status":"ok","timestamp":1555955148391,"user_tz":300,"elapsed":755,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["i = 0\n","while obs_seq[i] == '.':\n","  i = i + 1\n","# Now have start of first sentence at i\n","start_of = i\n","while obs_seq[i] != '.':\n","  i = i + 1\n","out_pos = obs_seq[start_of:i]\n","print(out_pos)\n","\n","out = {'pos_sequence': out_pos}\n","df = pd.DataFrame(out, columns= ['pos_sequence'])\n","df.to_csv ('/content/drive/My Drive/Colab Notebooks/out_pos.csv', index = None, header=True)\n","print(obs_seq)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['DT', 'JJ', 'JJ', 'NN', ',', 'NN']\n","['.', 'DT', 'JJ', 'JJ', 'NN', ',', 'NN', '.', 'DT', 'NN', 'MD', 'VB', 'IN', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'DT', 'NN', '.', 'DT', 'NN', 'VBZ', 'DT', 'JJ', 'NN', 'IN', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ']\n"],"name":"stdout"}]},{"metadata":{"id":"KWdcGZVDBkQW","colab_type":"code","outputId":"e73d72f8-718f-4c7d-b716-13bccdab368c","executionInfo":{"status":"error","timestamp":1555955350376,"user_tz":300,"elapsed":537,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"cell_type":"code","source":["\n","print(rnn.sample(ckpt_dir='/content/drive/My Drive/posmodel-100/', output_length=45))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c6ff5c371702>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouttie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobserved_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgo_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/posmodel-100/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: go_sample() missing 1 required positional argument: 'self'"]}]},{"metadata":{"id":"C45s-ddOhFs5","colab_type":"code","outputId":"e9b674af-9129-4d11-f17a-f1da5ab9cd2a","executionInfo":{"status":"ok","timestamp":1556498261954,"user_tz":300,"elapsed":30066,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["# BLACK BOX\n","np.random.seed(123)\n","rnn = CharRNN(len(pos), sampling=True)\n","#print(rnn.sample(ckpt_dir='/content/drive/My Drive/Colab Notebooks/model-100/', output_length=35))\n","obs_seq = rnn.sample(ckpt_dir='/content/drive/My Drive/posmodel-100/', output_length=45)\n","\n","i = 0\n","while obs_seq[i] == '.':\n","  i = i + 1\n","# Now have start of first sentence at i\n","start_of = i\n","while obs_seq[i] != '.':\n","  i = i + 1\n","out_pos = obs_seq[start_of:i]\n","print(out_pos)\n","\n","out = {'pos_sequence': out_pos}\n","df = pd.DataFrame(out, columns= ['pos_sequence'])\n","df.to_csv ('/content/drive/My Drive/Colab Notebooks/out_pos.csv', index = None, header=True)\n","print(obs_seq)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<< lstm_outputs >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 150), dtype=float32)\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/posmodel-100/language_modeling.ckpt\n",". DT NNS VBP DT NN NN IN DT NN , CC DT NN VBZ VBN IN NNS . DT NN NN IN DT NN NN VBZ VBN TO VB DT JJ NN , CC NN VBZ VBN IN DT NN IN PRP$ NNS . IN PRP \n","['DT', 'NNS', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNS']\n","['.', 'DT', 'NNS', 'VBP', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', ',', 'CC', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'NNS', '.', 'DT', 'NN', 'NN', 'IN', 'DT', 'NN', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', ',', 'CC', 'NN', 'VBZ', 'VBN', 'IN', 'DT', 'NN', 'IN', 'PRP$', 'NNS', '.', 'IN', 'PRP']\n"],"name":"stdout"}]},{"metadata":{"id":"Jjx8iS_0hjHo","colab_type":"code","colab":{}},"cell_type":"code","source":["##"],"execution_count":0,"outputs":[]}]}