{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"newCharRNNCombo.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Ac79ByeypHG0","colab_type":"code","outputId":"1cb17c9a-ad16-4169-bdd0-2abbd1e6d582","executionInfo":{"status":"ok","timestamp":1556499613371,"user_tz":300,"elapsed":154,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["import numpy as np\n","import torch\n","import pandas as pd\n","import csv\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","import random\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"EAQr025PaplW","colab_type":"code","outputId":"323e67f1-72d7-47b1-cf44-677be100ae6b","executionInfo":{"status":"ok","timestamp":1556499617506,"user_tz":300,"elapsed":2718,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","#sys.path.append('/content/drive/My Drive/Collij/STAT 479 Machine Learning/Deep Learning Project/DEEPakChopra/')\n","#sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n","sys.path.append('/content/drive/My Drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"PM1-EKhxarU2","colab_type":"code","outputId":"d884d477-84c7-4969-d2c2-c8cba64cd3d0","executionInfo":{"status":"ok","timestamp":1556499618259,"user_tz":300,"elapsed":229,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["########## BEGIN CharRNN ##########\n","# Character mapping to integers\n","#with open(\"/content/drive/My Drive/Collij/STAT 479 Machine Learning/Deep Learning Project/DEEPakChopra/cleanedTweets2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","\n","#with open(\"/content/drive/My Drive/cleanedTweets2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","with open(\"/content/drive/My Drive/Colab Notebooks/Deep Learning Project/joinedCleanedDataFinal2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","#with open(\"/content/drive/My Drive/cleanedTweets2.csv\", 'r', encoding='utf-8') as f:tweets=f.read()\n","\n","#dataframe = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/cleaned_book.csv\")  \n","#book_content_list = dataframe['data'].tolist()\n","\n","tweetChars = []\n","\n","tweetChars = set(tweets)\n","char2int = {ch:i for i,ch in enumerate(tweetChars)}\n","int2char = dict(enumerate(tweetChars))\n","text_ints = np.array([char2int[ch] for ch in tweets],dtype=np.int32)\n","#print(tweetChars)\n","print(text_ints)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[13 15 21 ... 25 28  5]\n"],"name":"stdout"}]},{"metadata":{"id":"Xr881jJvatHI","colab_type":"code","colab":{}},"cell_type":"code","source":["_lstm_size=250\n","_num_layers=2\n","_learning_rate=0.01\n","_keep_prob=0.6\n","_grad_clip=5\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eqXQRG_oaweO","colab_type":"code","colab":{}},"cell_type":"code","source":["batches = []\n","\n","# Function for splitting data\n","def split_data(sequence, batch_size, num_steps):\n","    total_length = batch_size * num_steps\n","    num_batches = int(len(sequence) / total_length)\n","    if num_batches*total_length + 1 > len(sequence):\n","        num_batches = num_batches - 1\n","    # Cut down character stream to length of a batch\n","    inputs = sequence[0: num_batches * total_length]\n","    output = sequence[1: num_batches * total_length + 1]\n","    # Split input & output:\n","    split_input = np.split(inputs, batch_size)\n","    split_output = np.split(output, batch_size)\n","    # Combine the batches\n","    inputs = np.stack(split_input)\n","    output = np.stack(split_output)\n","    return inputs, output\n","\n","\n","def create_batch_generator(data_x, data_y, num_steps):\n","    batch_size, total_length = data_x.shape\n","    num_batches = int(total_length/num_steps)\n","    for b in range(num_batches):\n","        yield (data_x[:, b*num_steps:(b+1)*num_steps],\n","               data_y[:, b*num_steps:(b+1)*num_steps])\n","\n","\n","def get_top_char(probas, char_size, top_n=5):\n","    p = np.squeeze(probas)\n","    p[np.argsort(p)[:-top_n]] = 0.0\n","    p = p / np.sum(p)\n","    ch_id = np.random.choice(char_size, 1, p=p)[0]\n","    return ch_id\n","\n","# Class for character level recurrent neural network\n","class CharRNN(object):\n","    # Constructor (note, the sampling parameter is for determining\n","    # what mode this object is in (training/sampling), and grad_clip\n","    # is for preventing exploding gradients\n","    def __init__(self, num_classes, batch_size=64,\n","                 num_steps=100, lstm_size = _lstm_size, #lstm_size=128,\n","                 num_layers = _num_layers, learning_rate = _learning_rate, #num_layers=1, learning_rate=0.001,\n","                 keep_prob = _keep_prob, grad_clip = _grad_clip, #keep_prob=0.5, grad_clip=5,\n","                 sampling=False):\n","        # Set variables to values given by parameters\n","        self.num_classes = num_classes\n","        self.batch_size = batch_size\n","        self.num_steps = num_steps\n","        self.lstm_size = lstm_size\n","        self.num_layers = num_layers\n","        self.learning_rate = learning_rate\n","        self.keep_prob = keep_prob\n","        self.grad_clip = grad_clip\n","        self.g = tf.Graph()\n","\n","        with self.g.as_default():\n","            tf.set_random_seed(123)\n","            self.build(sampling=sampling)  # builds x and y graphs of data\n","            self.init_op = tf.global_variables_initializer()\n","            self.saver = tf.train.Saver()\n","            \n","\n","    def build(self, sampling):\n","        if sampling:\n","            batch_size, num_steps = 1, 1\n","        else:\n","            batch_size = self.batch_size\n","            num_steps = self.num_steps\n","        tf_x = tf.placeholder(tf.int32,\n","                              shape=[batch_size, num_steps],\n","                              name='tf_x')\n","        tf_y = tf.placeholder(tf.int32,\n","                              shape=[batch_size, num_steps],\n","                              name='tf_y')\n","        tf_keepprob = tf.placeholder(tf.float32,\n","                                     name='tf_keepprob')\n","\n","        # One-hot encoding:\n","        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n","        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n","\n","        # Build the multi-layer RNN cells\n","        cells = tf.contrib.rnn.MultiRNNCell(\n","            [tf.contrib.rnn.DropoutWrapper(\n","                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n","                output_keep_prob=tf_keepprob)\n","            for _ in range(self.num_layers)])\n","\n","        # Define the initial state\n","        self.initial_state = cells.zero_state(\n","            batch_size, tf.float32)\n","\n","        # Run each sequence step through the RNN\n","        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n","            cells, x_onehot,\n","            initial_state=self.initial_state)\n","        print('<< lstm_outputs >>', lstm_outputs)\n","        seq_output_reshaped = tf.reshape(\n","            lstm_outputs,\n","            shape=[-1, self.lstm_size],\n","            name='seq_output_reshaped')\n","\n","        logits = tf.layers.dense(\n","            inputs=seq_output_reshaped,\n","            units=self.num_classes,\n","            activation=None,\n","            name='logits')\n","        probas = tf.nn.softmax(\n","            logits,\n","            name='probabilities')\n","\n","        y_reshaped = tf.reshape(\n","            y_onehot,\n","            shape=[-1, self.num_classes],\n","            name='y_reshaped')\n","        cost = tf.reduce_mean(\n","            tf.nn.softmax_cross_entropy_with_logits(\n","                logits=logits,\n","                labels=y_reshaped),\n","            name='cost')\n","\n","        # Gradient clipping to avoid \"exploding gradients\"\n","        tvars = tf.trainable_variables()\n","        grads, _ = tf.clip_by_global_norm(\n","            tf.gradients(cost, tvars),\n","            self.grad_clip)\n","        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n","        train_op = optimizer.apply_gradients(\n","            zip(grads, tvars),\n","            name='train_op')\n","\n","\n","    def train(self, train_x, train_y,\n","        num_epochs, ckpt_dir='./model/'):\n","\n","        # Create the checkpoint directory\n","        # if it does not exists\n","        if not os.path.exists(ckpt_dir):\n","            os.mkdir(ckpt_dir)\n","\n","        with tf.Session(graph=self.g) as sess:\n","            sess.run(self.init_op)\n","            n_batches = int(train_x.shape[1] / self.num_steps)\n","            iterations = n_batches * num_epochs\n","            for epoch in range(num_epochs):\n","\n","                # Train network\n","                new_state = sess.run(self.initial_state)\n","                loss = 0\n","\n","                # Mini-batch generator\n","                minibatchgen = create_batch_generator(\n","                    train_x, train_y, self.num_steps)\n","\n","                for b, (batch_x, batch_y) in enumerate(minibatchgen, 1):\n","                    iteration = epoch * n_batches + b\n","                    feed = {'tf_x:0': batch_x,\n","                            'tf_y:0': batch_y,\n","                            'tf_keepprob:0': self.keep_prob,\n","                            self.initial_state: new_state}\n","                    batch_cost, _, new_state = sess.run(\n","                        ['cost:0', 'train_op',\n","                         self.final_state],\n","                        feed_dict=feed)\n","                    if iteration % 10 == 0:\n","                        print('Epoch %d/%d Iteration %d'\n","                              '| Training loss: %.4f' % (\n","                                  epoch + 1, num_epochs,\n","                                  iteration, batch_cost))\n","                        batches.append(batch_cost)\n","\n","                # Save the trained model\n","                self.saver.save(\n","                    sess, os.path.join(\n","                        ckpt_dir, 'language_modeling.ckpt'))\n","\n","\n","    def sample(self, output_length,\n","               ckpt_dir, pospriors = ['NN','NN','NN','NN','NN']):\n","        #print(\"hi\")\n","        temp = list(tweetChars)\n","        randFirstLetter = temp[random.randint(0,len(tweetChars) - 1)]\n","        \n","        observed_seq = [randFirstLetter]\n","        starter_seq = [randFirstLetter]\n","        with tf.Session(graph=self.g) as sess:\n","            self.saver.restore(\n","            sess,\n","            tf.train.latest_checkpoint(ckpt_dir))\n","            \n","            # 1: run the model using the starter sequence\n","            new_state = sess.run(self.initial_state)\n","            #print(new_state)\n","            for ch in starter_seq:\n","                x = np.zeros((1, 1))\n","                x[0, 0] = char2int[ch.lower()]\n","                feed = {'tf_x:0': x,\n","                        'tf_keepprob:0': 1.0,\n","                        self.initial_state: new_state}\n","                probas, new_state = sess.run(\n","                            ['probabilities:0', self.final_state],\n","                            feed_dict=feed)\n","            ch_id = get_top_char(probas, len(tweetChars))\n","            observed_seq.append(int2char[ch_id])\n","\n","            # 2: run the model using the updated observed_seq\n","            end_of_last_word = 0\n","            word_pointer = 0\n","            i = 2\n","            while i < output_length and word_pointer < len(pospriors):\n","                x[0,0] = ch_id\n","                feed = {'tf_x:0': x,\n","                'tf_keepprob:0': 1.0,\n","                        self.initial_state: new_state}\n","                probas, new_state = sess.run(\n","                                ['probabilities:0', self.final_state],\n","                                feed_dict=feed)\n","\n","                ch_id = get_top_char(probas, len(tweetChars))\n","                # if we are moving to a new word...\n","                if ((int2char[ch_id] == \" \" or int2char[ch_id] == \"\\n\" or i == (output_length - 1))):\n","                  observed_seq.append(int2char[ch_id])\n","                  print(observed_seq[end_of_last_word:i])\n","                  word = ''.join(observed_seq[end_of_last_word:i])\n","                  print(\"Word generated: \",word)\n","                  \n","                  \n","                  if (len(nltk.pos_tag(nltk.word_tokenize(word))) != 0):\n","                    #print(nltk.pos_tag(nltk.word_tokenize(word)))\n","                    print(\"Looking for: \",pospriors[word_pointer])\n","                    print(\"Word found: \",nltk.pos_tag(nltk.word_tokenize(word))[0][1])\n","                    pos_of_word = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n","                    if (pos_of_word == pospriors[word_pointer]):\n","                      word_pointer = word_pointer + 1\n","                      end_of_last_word = i + 1 \n","                    else: \n","                      print(\"Failed\")\n","                      del observed_seq[end_of_last_word:len(observed_seq)]\n","                      i = end_of_last_word - 1\n","                      if (pospriors[word_pointer][1] == \",\"):\n","                        observed_seq.append(\", \")\n","                        word_pointer = word_pointer + 1\n","                  else: \n","                    i = i - 1\n","                else:\n","                  observed_seq.append(int2char[ch_id])\n","                i = i + 1\n","\n","        return ''.join(observed_seq)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ia7lScjuaxU6","colab_type":"code","outputId":"34a45d50-2e11-4448-912f-cfa58f007c3f","executionInfo":{"status":"ok","timestamp":1556499520820,"user_tz":300,"elapsed":1148863,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":22491}},"cell_type":"code","source":["# Begin executing CharRNN\n","batch_size = 64\n","num_steps = 100\n","train_x, train_y = split_data(text_ints,\n","                                batch_size,\n","                                num_steps)\n","rnn = CharRNN(num_classes=len(tweetChars), batch_size=batch_size)\n","rnn.train(train_x, train_y,\n","          num_epochs=90,\n","          ckpt_dir='/content/drive/My Drive/model-100/')\n","\n","del rnn\n","\n","np.random.seed(123)\n","rnn = CharRNN(len(tweetChars), sampling=True)\n","#print(rnn.sample(ckpt_dir='/content/drive/My Drive/Colab Notebooks/model-100/', output_length=35))\n","print(rnn.sample(ckpt_dir='/content/drive/My Drive/model-100/', output_length=70,pospriors = ['DT', 'NN', 'VBZ', 'JJ', 'CC', 'RB', 'DT', 'NN', 'IN', 'NN']))\n","\n","plt.plot(batches)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-5-913a9334b812>:88: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-5-913a9334b812>:88: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-5-913a9334b812>:97: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","<< lstm_outputs >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 250), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-5-913a9334b812>:108: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From <ipython-input-5-913a9334b812>:120: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","Epoch 1/90 Iteration 10| Training loss: 3.0011\n","Epoch 1/90 Iteration 20| Training loss: 2.9277\n","Epoch 1/90 Iteration 30| Training loss: 2.9058\n","Epoch 1/90 Iteration 40| Training loss: 2.8564\n","Epoch 1/90 Iteration 50| Training loss: 2.7591\n","Epoch 1/90 Iteration 60| Training loss: 2.7045\n","Epoch 1/90 Iteration 70| Training loss: 2.5816\n","Epoch 1/90 Iteration 80| Training loss: 2.5944\n","Epoch 1/90 Iteration 90| Training loss: 2.4806\n","Epoch 2/90 Iteration 100| Training loss: 2.4176\n","Epoch 2/90 Iteration 110| Training loss: 2.3358\n","Epoch 2/90 Iteration 120| Training loss: 2.2821\n","Epoch 2/90 Iteration 130| Training loss: 2.2514\n","Epoch 2/90 Iteration 140| Training loss: 2.2169\n","Epoch 2/90 Iteration 150| Training loss: 2.1757\n","Epoch 2/90 Iteration 160| Training loss: 2.1255\n","Epoch 2/90 Iteration 170| Training loss: 2.0782\n","Epoch 2/90 Iteration 180| Training loss: 2.0636\n","Epoch 2/90 Iteration 190| Training loss: 2.0234\n","Epoch 3/90 Iteration 200| Training loss: 1.9510\n","Epoch 3/90 Iteration 210| Training loss: 1.9601\n","Epoch 3/90 Iteration 220| Training loss: 1.9450\n","Epoch 3/90 Iteration 230| Training loss: 1.9250\n","Epoch 3/90 Iteration 240| Training loss: 1.8513\n","Epoch 3/90 Iteration 250| Training loss: 1.8399\n","Epoch 3/90 Iteration 260| Training loss: 1.8145\n","Epoch 3/90 Iteration 270| Training loss: 1.7792\n","Epoch 3/90 Iteration 280| Training loss: 1.7889\n","Epoch 4/90 Iteration 290| Training loss: 1.8156\n","Epoch 4/90 Iteration 300| Training loss: 1.7926\n","Epoch 4/90 Iteration 310| Training loss: 1.7163\n","Epoch 4/90 Iteration 320| Training loss: 1.7488\n","Epoch 4/90 Iteration 330| Training loss: 1.6745\n","Epoch 4/90 Iteration 340| Training loss: 1.7098\n","Epoch 4/90 Iteration 350| Training loss: 1.7054\n","Epoch 4/90 Iteration 360| Training loss: 1.7081\n","Epoch 4/90 Iteration 370| Training loss: 1.7312\n","Epoch 4/90 Iteration 380| Training loss: 1.7153\n","Epoch 5/90 Iteration 390| Training loss: 1.7127\n","Epoch 5/90 Iteration 400| Training loss: 1.6235\n","Epoch 5/90 Iteration 410| Training loss: 1.6134\n","Epoch 5/90 Iteration 420| Training loss: 1.5823\n","Epoch 5/90 Iteration 430| Training loss: 1.5604\n","Epoch 5/90 Iteration 440| Training loss: 1.6131\n","Epoch 5/90 Iteration 450| Training loss: 1.5837\n","Epoch 5/90 Iteration 460| Training loss: 1.6099\n","Epoch 5/90 Iteration 470| Training loss: 1.5654\n","Epoch 5/90 Iteration 480| Training loss: 1.5665\n","Epoch 6/90 Iteration 490| Training loss: 1.5418\n","Epoch 6/90 Iteration 500| Training loss: 1.5801\n","Epoch 6/90 Iteration 510| Training loss: 1.5846\n","Epoch 6/90 Iteration 520| Training loss: 1.6101\n","Epoch 6/90 Iteration 530| Training loss: 1.5600\n","Epoch 6/90 Iteration 540| Training loss: 1.5828\n","Epoch 6/90 Iteration 550| Training loss: 1.5513\n","Epoch 6/90 Iteration 560| Training loss: 1.6040\n","Epoch 6/90 Iteration 570| Training loss: 1.5240\n","Epoch 7/90 Iteration 580| Training loss: 1.5321\n","Epoch 7/90 Iteration 590| Training loss: 1.5240\n","Epoch 7/90 Iteration 600| Training loss: 1.5067\n","Epoch 7/90 Iteration 610| Training loss: 1.5243\n","Epoch 7/90 Iteration 620| Training loss: 1.4825\n","Epoch 7/90 Iteration 630| Training loss: 1.5327\n","Epoch 7/90 Iteration 640| Training loss: 1.5190\n","Epoch 7/90 Iteration 650| Training loss: 1.4708\n","Epoch 7/90 Iteration 660| Training loss: 1.5407\n","Epoch 7/90 Iteration 670| Training loss: 1.5027\n","Epoch 8/90 Iteration 680| Training loss: 1.4473\n","Epoch 8/90 Iteration 690| Training loss: 1.4890\n","Epoch 8/90 Iteration 700| Training loss: 1.4858\n","Epoch 8/90 Iteration 710| Training loss: 1.4957\n","Epoch 8/90 Iteration 720| Training loss: 1.4799\n","Epoch 8/90 Iteration 730| Training loss: 1.4630\n","Epoch 8/90 Iteration 740| Training loss: 1.4536\n","Epoch 8/90 Iteration 750| Training loss: 1.4435\n","Epoch 8/90 Iteration 760| Training loss: 1.4713\n","Epoch 9/90 Iteration 770| Training loss: 1.4868\n","Epoch 9/90 Iteration 780| Training loss: 1.4790\n","Epoch 9/90 Iteration 790| Training loss: 1.4328\n","Epoch 9/90 Iteration 800| Training loss: 1.4942\n","Epoch 9/90 Iteration 810| Training loss: 1.4138\n","Epoch 9/90 Iteration 820| Training loss: 1.4659\n","Epoch 9/90 Iteration 830| Training loss: 1.4700\n","Epoch 9/90 Iteration 840| Training loss: 1.4658\n","Epoch 9/90 Iteration 850| Training loss: 1.5064\n","Epoch 9/90 Iteration 860| Training loss: 1.4894\n","Epoch 10/90 Iteration 870| Training loss: 1.4916\n","Epoch 10/90 Iteration 880| Training loss: 1.4461\n","Epoch 10/90 Iteration 890| Training loss: 1.4257\n","Epoch 10/90 Iteration 900| Training loss: 1.3987\n","Epoch 10/90 Iteration 910| Training loss: 1.3727\n","Epoch 10/90 Iteration 920| Training loss: 1.4304\n","Epoch 10/90 Iteration 930| Training loss: 1.4248\n","Epoch 10/90 Iteration 940| Training loss: 1.4464\n","Epoch 10/90 Iteration 950| Training loss: 1.3993\n","Epoch 10/90 Iteration 960| Training loss: 1.4154\n","Epoch 11/90 Iteration 970| Training loss: 1.3879\n","Epoch 11/90 Iteration 980| Training loss: 1.4388\n","Epoch 11/90 Iteration 990| Training loss: 1.4427\n","Epoch 11/90 Iteration 1000| Training loss: 1.4465\n","Epoch 11/90 Iteration 1010| Training loss: 1.4231\n","Epoch 11/90 Iteration 1020| Training loss: 1.4403\n","Epoch 11/90 Iteration 1030| Training loss: 1.4276\n","Epoch 11/90 Iteration 1040| Training loss: 1.4612\n","Epoch 11/90 Iteration 1050| Training loss: 1.3769\n","Epoch 12/90 Iteration 1060| Training loss: 1.4058\n","Epoch 12/90 Iteration 1070| Training loss: 1.4041\n","Epoch 12/90 Iteration 1080| Training loss: 1.3791\n","Epoch 12/90 Iteration 1090| Training loss: 1.4151\n","Epoch 12/90 Iteration 1100| Training loss: 1.3779\n","Epoch 12/90 Iteration 1110| Training loss: 1.4067\n","Epoch 12/90 Iteration 1120| Training loss: 1.4279\n","Epoch 12/90 Iteration 1130| Training loss: 1.3810\n","Epoch 12/90 Iteration 1140| Training loss: 1.4276\n","Epoch 12/90 Iteration 1150| Training loss: 1.3959\n","Epoch 13/90 Iteration 1160| Training loss: 1.3580\n","Epoch 13/90 Iteration 1170| Training loss: 1.3894\n","Epoch 13/90 Iteration 1180| Training loss: 1.3758\n","Epoch 13/90 Iteration 1190| Training loss: 1.4011\n","Epoch 13/90 Iteration 1200| Training loss: 1.3812\n","Epoch 13/90 Iteration 1210| Training loss: 1.3681\n","Epoch 13/90 Iteration 1220| Training loss: 1.3620\n","Epoch 13/90 Iteration 1230| Training loss: 1.3569\n","Epoch 13/90 Iteration 1240| Training loss: 1.3807\n","Epoch 14/90 Iteration 1250| Training loss: 1.3869\n","Epoch 14/90 Iteration 1260| Training loss: 1.3959\n","Epoch 14/90 Iteration 1270| Training loss: 1.3550\n","Epoch 14/90 Iteration 1280| Training loss: 1.4181\n","Epoch 14/90 Iteration 1290| Training loss: 1.3334\n","Epoch 14/90 Iteration 1300| Training loss: 1.3786\n","Epoch 14/90 Iteration 1310| Training loss: 1.3905\n","Epoch 14/90 Iteration 1320| Training loss: 1.3802\n","Epoch 14/90 Iteration 1330| Training loss: 1.4304\n","Epoch 14/90 Iteration 1340| Training loss: 1.3985\n","Epoch 15/90 Iteration 1350| Training loss: 1.4275\n","Epoch 15/90 Iteration 1360| Training loss: 1.3708\n","Epoch 15/90 Iteration 1370| Training loss: 1.3652\n","Epoch 15/90 Iteration 1380| Training loss: 1.3174\n","Epoch 15/90 Iteration 1390| Training loss: 1.3125\n","Epoch 15/90 Iteration 1400| Training loss: 1.3563\n","Epoch 15/90 Iteration 1410| Training loss: 1.3400\n","Epoch 15/90 Iteration 1420| Training loss: 1.3840\n","Epoch 15/90 Iteration 1430| Training loss: 1.3366\n","Epoch 15/90 Iteration 1440| Training loss: 1.3596\n","Epoch 16/90 Iteration 1450| Training loss: 1.3251\n","Epoch 16/90 Iteration 1460| Training loss: 1.3728\n","Epoch 16/90 Iteration 1470| Training loss: 1.3685\n","Epoch 16/90 Iteration 1480| Training loss: 1.3785\n","Epoch 16/90 Iteration 1490| Training loss: 1.3547\n","Epoch 16/90 Iteration 1500| Training loss: 1.3883\n","Epoch 16/90 Iteration 1510| Training loss: 1.3710\n","Epoch 16/90 Iteration 1520| Training loss: 1.3931\n","Epoch 16/90 Iteration 1530| Training loss: 1.3401\n","Epoch 17/90 Iteration 1540| Training loss: 1.3386\n","Epoch 17/90 Iteration 1550| Training loss: 1.3524\n","Epoch 17/90 Iteration 1560| Training loss: 1.3281\n","Epoch 17/90 Iteration 1570| Training loss: 1.3635\n","Epoch 17/90 Iteration 1580| Training loss: 1.3244\n","Epoch 17/90 Iteration 1590| Training loss: 1.3443\n","Epoch 17/90 Iteration 1600| Training loss: 1.3558\n","Epoch 17/90 Iteration 1610| Training loss: 1.3358\n","Epoch 17/90 Iteration 1620| Training loss: 1.3805\n","Epoch 17/90 Iteration 1630| Training loss: 1.3428\n","Epoch 18/90 Iteration 1640| Training loss: 1.3126\n","Epoch 18/90 Iteration 1650| Training loss: 1.3573\n","Epoch 18/90 Iteration 1660| Training loss: 1.3256\n","Epoch 18/90 Iteration 1670| Training loss: 1.3571\n","Epoch 18/90 Iteration 1680| Training loss: 1.3269\n","Epoch 18/90 Iteration 1690| Training loss: 1.3299\n","Epoch 18/90 Iteration 1700| Training loss: 1.3316\n","Epoch 18/90 Iteration 1710| Training loss: 1.3195\n","Epoch 18/90 Iteration 1720| Training loss: 1.3175\n","Epoch 19/90 Iteration 1730| Training loss: 1.3409\n","Epoch 19/90 Iteration 1740| Training loss: 1.3617\n","Epoch 19/90 Iteration 1750| Training loss: 1.2986\n","Epoch 19/90 Iteration 1760| Training loss: 1.3775\n","Epoch 19/90 Iteration 1770| Training loss: 1.2931\n","Epoch 19/90 Iteration 1780| Training loss: 1.3257\n","Epoch 19/90 Iteration 1790| Training loss: 1.3543\n","Epoch 19/90 Iteration 1800| Training loss: 1.3347\n","Epoch 19/90 Iteration 1810| Training loss: 1.3794\n","Epoch 19/90 Iteration 1820| Training loss: 1.3631\n","Epoch 20/90 Iteration 1830| Training loss: 1.3788\n","Epoch 20/90 Iteration 1840| Training loss: 1.3230\n","Epoch 20/90 Iteration 1850| Training loss: 1.3165\n","Epoch 20/90 Iteration 1860| Training loss: 1.2870\n","Epoch 20/90 Iteration 1870| Training loss: 1.2738\n","Epoch 20/90 Iteration 1880| Training loss: 1.3151\n","Epoch 20/90 Iteration 1890| Training loss: 1.2881\n","Epoch 20/90 Iteration 1900| Training loss: 1.3394\n","Epoch 20/90 Iteration 1910| Training loss: 1.2962\n","Epoch 20/90 Iteration 1920| Training loss: 1.3059\n","Epoch 21/90 Iteration 1930| Training loss: 1.2884\n","Epoch 21/90 Iteration 1940| Training loss: 1.3392\n","Epoch 21/90 Iteration 1950| Training loss: 1.3311\n","Epoch 21/90 Iteration 1960| Training loss: 1.3473\n","Epoch 21/90 Iteration 1970| Training loss: 1.3234\n","Epoch 21/90 Iteration 1980| Training loss: 1.3512\n","Epoch 21/90 Iteration 1990| Training loss: 1.3367\n","Epoch 21/90 Iteration 2000| Training loss: 1.3722\n","Epoch 21/90 Iteration 2010| Training loss: 1.3018\n","Epoch 22/90 Iteration 2020| Training loss: 1.3122\n","Epoch 22/90 Iteration 2030| Training loss: 1.3206\n","Epoch 22/90 Iteration 2040| Training loss: 1.2893\n","Epoch 22/90 Iteration 2050| Training loss: 1.3295\n","Epoch 22/90 Iteration 2060| Training loss: 1.2840\n","Epoch 22/90 Iteration 2070| Training loss: 1.3114\n","Epoch 22/90 Iteration 2080| Training loss: 1.3316\n","Epoch 22/90 Iteration 2090| Training loss: 1.2924\n","Epoch 22/90 Iteration 2100| Training loss: 1.3538\n","Epoch 22/90 Iteration 2110| Training loss: 1.3050\n","Epoch 23/90 Iteration 2120| Training loss: 1.2860\n","Epoch 23/90 Iteration 2130| Training loss: 1.3057\n","Epoch 23/90 Iteration 2140| Training loss: 1.2915\n","Epoch 23/90 Iteration 2150| Training loss: 1.3165\n","Epoch 23/90 Iteration 2160| Training loss: 1.2995\n","Epoch 23/90 Iteration 2170| Training loss: 1.2993\n","Epoch 23/90 Iteration 2180| Training loss: 1.2850\n","Epoch 23/90 Iteration 2190| Training loss: 1.2836\n","Epoch 23/90 Iteration 2200| Training loss: 1.2986\n","Epoch 24/90 Iteration 2210| Training loss: 1.3091\n","Epoch 24/90 Iteration 2220| Training loss: 1.3183\n","Epoch 24/90 Iteration 2230| Training loss: 1.2707\n","Epoch 24/90 Iteration 2240| Training loss: 1.3400\n","Epoch 24/90 Iteration 2250| Training loss: 1.2615\n","Epoch 24/90 Iteration 2260| Training loss: 1.2979\n","Epoch 24/90 Iteration 2270| Training loss: 1.3244\n","Epoch 24/90 Iteration 2280| Training loss: 1.3027\n","Epoch 24/90 Iteration 2290| Training loss: 1.3559\n","Epoch 24/90 Iteration 2300| Training loss: 1.3344\n","Epoch 25/90 Iteration 2310| Training loss: 1.3522\n","Epoch 25/90 Iteration 2320| Training loss: 1.3058\n","Epoch 25/90 Iteration 2330| Training loss: 1.2831\n","Epoch 25/90 Iteration 2340| Training loss: 1.2542\n","Epoch 25/90 Iteration 2350| Training loss: 1.2449\n","Epoch 25/90 Iteration 2360| Training loss: 1.2949\n","Epoch 25/90 Iteration 2370| Training loss: 1.2752\n","Epoch 25/90 Iteration 2380| Training loss: 1.3131\n","Epoch 25/90 Iteration 2390| Training loss: 1.2753\n","Epoch 25/90 Iteration 2400| Training loss: 1.2869\n","Epoch 26/90 Iteration 2410| Training loss: 1.2576\n","Epoch 26/90 Iteration 2420| Training loss: 1.2995\n","Epoch 26/90 Iteration 2430| Training loss: 1.2944\n","Epoch 26/90 Iteration 2440| Training loss: 1.3160\n","Epoch 26/90 Iteration 2450| Training loss: 1.2901\n","Epoch 26/90 Iteration 2460| Training loss: 1.3051\n","Epoch 26/90 Iteration 2470| Training loss: 1.3087\n","Epoch 26/90 Iteration 2480| Training loss: 1.3483\n","Epoch 26/90 Iteration 2490| Training loss: 1.2916\n","Epoch 27/90 Iteration 2500| Training loss: 1.2814\n","Epoch 27/90 Iteration 2510| Training loss: 1.3020\n","Epoch 27/90 Iteration 2520| Training loss: 1.2779\n","Epoch 27/90 Iteration 2530| Training loss: 1.2917\n","Epoch 27/90 Iteration 2540| Training loss: 1.2632\n","Epoch 27/90 Iteration 2550| Training loss: 1.2936\n","Epoch 27/90 Iteration 2560| Training loss: 1.3066\n","Epoch 27/90 Iteration 2570| Training loss: 1.2696\n","Epoch 27/90 Iteration 2580| Training loss: 1.3357\n","Epoch 27/90 Iteration 2590| Training loss: 1.2914\n","Epoch 28/90 Iteration 2600| Training loss: 1.2680\n","Epoch 28/90 Iteration 2610| Training loss: 1.3026\n","Epoch 28/90 Iteration 2620| Training loss: 1.2691\n","Epoch 28/90 Iteration 2630| Training loss: 1.2764\n","Epoch 28/90 Iteration 2640| Training loss: 1.2822\n","Epoch 28/90 Iteration 2650| Training loss: 1.2661\n","Epoch 28/90 Iteration 2660| Training loss: 1.2684\n","Epoch 28/90 Iteration 2670| Training loss: 1.2648\n","Epoch 28/90 Iteration 2680| Training loss: 1.2831\n","Epoch 29/90 Iteration 2690| Training loss: 1.3051\n","Epoch 29/90 Iteration 2700| Training loss: 1.2963\n","Epoch 29/90 Iteration 2710| Training loss: 1.2694\n","Epoch 29/90 Iteration 2720| Training loss: 1.3184\n","Epoch 29/90 Iteration 2730| Training loss: 1.2308\n","Epoch 29/90 Iteration 2740| Training loss: 1.2735\n","Epoch 29/90 Iteration 2750| Training loss: 1.3136\n","Epoch 29/90 Iteration 2760| Training loss: 1.2978\n","Epoch 29/90 Iteration 2770| Training loss: 1.3474\n","Epoch 29/90 Iteration 2780| Training loss: 1.3094\n","Epoch 30/90 Iteration 2790| Training loss: 1.3269\n","Epoch 30/90 Iteration 2800| Training loss: 1.2748\n","Epoch 30/90 Iteration 2810| Training loss: 1.2787\n","Epoch 30/90 Iteration 2820| Training loss: 1.2387\n","Epoch 30/90 Iteration 2830| Training loss: 1.2361\n","Epoch 30/90 Iteration 2840| Training loss: 1.2644\n","Epoch 30/90 Iteration 2850| Training loss: 1.2676\n","Epoch 30/90 Iteration 2860| Training loss: 1.3015\n","Epoch 30/90 Iteration 2870| Training loss: 1.2606\n","Epoch 30/90 Iteration 2880| Training loss: 1.2754\n","Epoch 31/90 Iteration 2890| Training loss: 1.2442\n","Epoch 31/90 Iteration 2900| Training loss: 1.2991\n","Epoch 31/90 Iteration 2910| Training loss: 1.2745\n","Epoch 31/90 Iteration 2920| Training loss: 1.3010\n","Epoch 31/90 Iteration 2930| Training loss: 1.2734\n","Epoch 31/90 Iteration 2940| Training loss: 1.2985\n","Epoch 31/90 Iteration 2950| Training loss: 1.2818\n","Epoch 31/90 Iteration 2960| Training loss: 1.3281\n","Epoch 31/90 Iteration 2970| Training loss: 1.2484\n","Epoch 32/90 Iteration 2980| Training loss: 1.2784\n","Epoch 32/90 Iteration 2990| Training loss: 1.2783\n","Epoch 32/90 Iteration 3000| Training loss: 1.2535\n","Epoch 32/90 Iteration 3010| Training loss: 1.2839\n","Epoch 32/90 Iteration 3020| Training loss: 1.2503\n","Epoch 32/90 Iteration 3030| Training loss: 1.2665\n","Epoch 32/90 Iteration 3040| Training loss: 1.2810\n","Epoch 32/90 Iteration 3050| Training loss: 1.2500\n","Epoch 32/90 Iteration 3060| Training loss: 1.3117\n","Epoch 32/90 Iteration 3070| Training loss: 1.2688\n","Epoch 33/90 Iteration 3080| Training loss: 1.2472\n","Epoch 33/90 Iteration 3090| Training loss: 1.2772\n","Epoch 33/90 Iteration 3100| Training loss: 1.2533\n","Epoch 33/90 Iteration 3110| Training loss: 1.2719\n","Epoch 33/90 Iteration 3120| Training loss: 1.2581\n","Epoch 33/90 Iteration 3130| Training loss: 1.2481\n","Epoch 33/90 Iteration 3140| Training loss: 1.2456\n","Epoch 33/90 Iteration 3150| Training loss: 1.2668\n","Epoch 33/90 Iteration 3160| Training loss: 1.2633\n","Epoch 34/90 Iteration 3170| Training loss: 1.2846\n","Epoch 34/90 Iteration 3180| Training loss: 1.2859\n","Epoch 34/90 Iteration 3190| Training loss: 1.2461\n","Epoch 34/90 Iteration 3200| Training loss: 1.3058\n","Epoch 34/90 Iteration 3210| Training loss: 1.2224\n","Epoch 34/90 Iteration 3220| Training loss: 1.2721\n","Epoch 34/90 Iteration 3230| Training loss: 1.2786\n","Epoch 34/90 Iteration 3240| Training loss: 1.2849\n","Epoch 34/90 Iteration 3250| Training loss: 1.3361\n","Epoch 34/90 Iteration 3260| Training loss: 1.2974\n","Epoch 35/90 Iteration 3270| Training loss: 1.3149\n","Epoch 35/90 Iteration 3280| Training loss: 1.2639\n","Epoch 35/90 Iteration 3290| Training loss: 1.2593\n","Epoch 35/90 Iteration 3300| Training loss: 1.2338\n","Epoch 35/90 Iteration 3310| Training loss: 1.2200\n","Epoch 35/90 Iteration 3320| Training loss: 1.2606\n","Epoch 35/90 Iteration 3330| Training loss: 1.2347\n","Epoch 35/90 Iteration 3340| Training loss: 1.2741\n","Epoch 35/90 Iteration 3350| Training loss: 1.2321\n","Epoch 35/90 Iteration 3360| Training loss: 1.2509\n","Epoch 36/90 Iteration 3370| Training loss: 1.2267\n","Epoch 36/90 Iteration 3380| Training loss: 1.2717\n","Epoch 36/90 Iteration 3390| Training loss: 1.2625\n","Epoch 36/90 Iteration 3400| Training loss: 1.2847\n","Epoch 36/90 Iteration 3410| Training loss: 1.2479\n","Epoch 36/90 Iteration 3420| Training loss: 1.2978\n","Epoch 36/90 Iteration 3430| Training loss: 1.2781\n","Epoch 36/90 Iteration 3440| Training loss: 1.3028\n","Epoch 36/90 Iteration 3450| Training loss: 1.2489\n","Epoch 37/90 Iteration 3460| Training loss: 1.2600\n","Epoch 37/90 Iteration 3470| Training loss: 1.2637\n","Epoch 37/90 Iteration 3480| Training loss: 1.2348\n","Epoch 37/90 Iteration 3490| Training loss: 1.2625\n","Epoch 37/90 Iteration 3500| Training loss: 1.2395\n","Epoch 37/90 Iteration 3510| Training loss: 1.2521\n","Epoch 37/90 Iteration 3520| Training loss: 1.2657\n","Epoch 37/90 Iteration 3530| Training loss: 1.2252\n","Epoch 37/90 Iteration 3540| Training loss: 1.2925\n","Epoch 37/90 Iteration 3550| Training loss: 1.2701\n","Epoch 38/90 Iteration 3560| Training loss: 1.2410\n","Epoch 38/90 Iteration 3570| Training loss: 1.2595\n","Epoch 38/90 Iteration 3580| Training loss: 1.2482\n","Epoch 38/90 Iteration 3590| Training loss: 1.2585\n","Epoch 38/90 Iteration 3600| Training loss: 1.2413\n","Epoch 38/90 Iteration 3610| Training loss: 1.2552\n","Epoch 38/90 Iteration 3620| Training loss: 1.2486\n","Epoch 38/90 Iteration 3630| Training loss: 1.2400\n","Epoch 38/90 Iteration 3640| Training loss: 1.2339\n","Epoch 39/90 Iteration 3650| Training loss: 1.2700\n","Epoch 39/90 Iteration 3660| Training loss: 1.2687\n","Epoch 39/90 Iteration 3670| Training loss: 1.2310\n","Epoch 39/90 Iteration 3680| Training loss: 1.2934\n","Epoch 39/90 Iteration 3690| Training loss: 1.2133\n","Epoch 39/90 Iteration 3700| Training loss: 1.2536\n","Epoch 39/90 Iteration 3710| Training loss: 1.2621\n","Epoch 39/90 Iteration 3720| Training loss: 1.2599\n","Epoch 39/90 Iteration 3730| Training loss: 1.3093\n","Epoch 39/90 Iteration 3740| Training loss: 1.2889\n","Epoch 40/90 Iteration 3750| Training loss: 1.2996\n","Epoch 40/90 Iteration 3760| Training loss: 1.2500\n","Epoch 40/90 Iteration 3770| Training loss: 1.2413\n","Epoch 40/90 Iteration 3780| Training loss: 1.2122\n","Epoch 40/90 Iteration 3790| Training loss: 1.2076\n","Epoch 40/90 Iteration 3800| Training loss: 1.2461\n","Epoch 40/90 Iteration 3810| Training loss: 1.2209\n","Epoch 40/90 Iteration 3820| Training loss: 1.2736\n","Epoch 40/90 Iteration 3830| Training loss: 1.2414\n","Epoch 40/90 Iteration 3840| Training loss: 1.2320\n","Epoch 41/90 Iteration 3850| Training loss: 1.2223\n","Epoch 41/90 Iteration 3860| Training loss: 1.2447\n","Epoch 41/90 Iteration 3870| Training loss: 1.2488\n","Epoch 41/90 Iteration 3880| Training loss: 1.2959\n","Epoch 41/90 Iteration 3890| Training loss: 1.2433\n","Epoch 41/90 Iteration 3900| Training loss: 1.2785\n","Epoch 41/90 Iteration 3910| Training loss: 1.2808\n","Epoch 41/90 Iteration 3920| Training loss: 1.2867\n","Epoch 41/90 Iteration 3930| Training loss: 1.2410\n","Epoch 42/90 Iteration 3940| Training loss: 1.2468\n","Epoch 42/90 Iteration 3950| Training loss: 1.2478\n","Epoch 42/90 Iteration 3960| Training loss: 1.2289\n","Epoch 42/90 Iteration 3970| Training loss: 1.2574\n","Epoch 42/90 Iteration 3980| Training loss: 1.2188\n","Epoch 42/90 Iteration 3990| Training loss: 1.2503\n","Epoch 42/90 Iteration 4000| Training loss: 1.2562\n","Epoch 42/90 Iteration 4010| Training loss: 1.2130\n","Epoch 42/90 Iteration 4020| Training loss: 1.2893\n","Epoch 42/90 Iteration 4030| Training loss: 1.2504\n","Epoch 43/90 Iteration 4040| Training loss: 1.2198\n","Epoch 43/90 Iteration 4050| Training loss: 1.2489\n","Epoch 43/90 Iteration 4060| Training loss: 1.2234\n","Epoch 43/90 Iteration 4070| Training loss: 1.2532\n","Epoch 43/90 Iteration 4080| Training loss: 1.2334\n","Epoch 43/90 Iteration 4090| Training loss: 1.2401\n","Epoch 43/90 Iteration 4100| Training loss: 1.2275\n","Epoch 43/90 Iteration 4110| Training loss: 1.2274\n","Epoch 43/90 Iteration 4120| Training loss: 1.2518\n","Epoch 44/90 Iteration 4130| Training loss: 1.2570\n","Epoch 44/90 Iteration 4140| Training loss: 1.2630\n","Epoch 44/90 Iteration 4150| Training loss: 1.2282\n","Epoch 44/90 Iteration 4160| Training loss: 1.2903\n","Epoch 44/90 Iteration 4170| Training loss: 1.1965\n","Epoch 44/90 Iteration 4180| Training loss: 1.2302\n","Epoch 44/90 Iteration 4190| Training loss: 1.2672\n","Epoch 44/90 Iteration 4200| Training loss: 1.2457\n","Epoch 44/90 Iteration 4210| Training loss: 1.3124\n","Epoch 44/90 Iteration 4220| Training loss: 1.2755\n","Epoch 45/90 Iteration 4230| Training loss: 1.2952\n","Epoch 45/90 Iteration 4240| Training loss: 1.2481\n","Epoch 45/90 Iteration 4250| Training loss: 1.2280\n","Epoch 45/90 Iteration 4260| Training loss: 1.1920\n","Epoch 45/90 Iteration 4270| Training loss: 1.1948\n","Epoch 45/90 Iteration 4280| Training loss: 1.2339\n","Epoch 45/90 Iteration 4290| Training loss: 1.2130\n","Epoch 45/90 Iteration 4300| Training loss: 1.2506\n","Epoch 45/90 Iteration 4310| Training loss: 1.2212\n","Epoch 45/90 Iteration 4320| Training loss: 1.2283\n","Epoch 46/90 Iteration 4330| Training loss: 1.2145\n","Epoch 46/90 Iteration 4340| Training loss: 1.2493\n","Epoch 46/90 Iteration 4350| Training loss: 1.2283\n","Epoch 46/90 Iteration 4360| Training loss: 1.2738\n","Epoch 46/90 Iteration 4370| Training loss: 1.2276\n","Epoch 46/90 Iteration 4380| Training loss: 1.2625\n","Epoch 46/90 Iteration 4390| Training loss: 1.2611\n","Epoch 46/90 Iteration 4400| Training loss: 1.2712\n","Epoch 46/90 Iteration 4410| Training loss: 1.2248\n","Epoch 47/90 Iteration 4420| Training loss: 1.2236\n","Epoch 47/90 Iteration 4430| Training loss: 1.2410\n","Epoch 47/90 Iteration 4440| Training loss: 1.2154\n","Epoch 47/90 Iteration 4450| Training loss: 1.2515\n","Epoch 47/90 Iteration 4460| Training loss: 1.2113\n","Epoch 47/90 Iteration 4470| Training loss: 1.2408\n","Epoch 47/90 Iteration 4480| Training loss: 1.2532\n","Epoch 47/90 Iteration 4490| Training loss: 1.2227\n","Epoch 47/90 Iteration 4500| Training loss: 1.2782\n","Epoch 47/90 Iteration 4510| Training loss: 1.2322\n","Epoch 48/90 Iteration 4520| Training loss: 1.2254\n","Epoch 48/90 Iteration 4530| Training loss: 1.2443\n","Epoch 48/90 Iteration 4540| Training loss: 1.2202\n","Epoch 48/90 Iteration 4550| Training loss: 1.2444\n","Epoch 48/90 Iteration 4560| Training loss: 1.2305\n","Epoch 48/90 Iteration 4570| Training loss: 1.2256\n","Epoch 48/90 Iteration 4580| Training loss: 1.2134\n","Epoch 48/90 Iteration 4590| Training loss: 1.2189\n","Epoch 48/90 Iteration 4600| Training loss: 1.2242\n","Epoch 49/90 Iteration 4610| Training loss: 1.2421\n","Epoch 49/90 Iteration 4620| Training loss: 1.2554\n","Epoch 49/90 Iteration 4630| Training loss: 1.2140\n","Epoch 49/90 Iteration 4640| Training loss: 1.2782\n","Epoch 49/90 Iteration 4650| Training loss: 1.1828\n","Epoch 49/90 Iteration 4660| Training loss: 1.2501\n","Epoch 49/90 Iteration 4670| Training loss: 1.2498\n","Epoch 49/90 Iteration 4680| Training loss: 1.2515\n","Epoch 49/90 Iteration 4690| Training loss: 1.2837\n","Epoch 49/90 Iteration 4700| Training loss: 1.2647\n","Epoch 50/90 Iteration 4710| Training loss: 1.2699\n","Epoch 50/90 Iteration 4720| Training loss: 1.2313\n","Epoch 50/90 Iteration 4730| Training loss: 1.2273\n","Epoch 50/90 Iteration 4740| Training loss: 1.2087\n","Epoch 50/90 Iteration 4750| Training loss: 1.1841\n","Epoch 50/90 Iteration 4760| Training loss: 1.2304\n","Epoch 50/90 Iteration 4770| Training loss: 1.2195\n","Epoch 50/90 Iteration 4780| Training loss: 1.2506\n","Epoch 50/90 Iteration 4790| Training loss: 1.2209\n","Epoch 50/90 Iteration 4800| Training loss: 1.2178\n","Epoch 51/90 Iteration 4810| Training loss: 1.2108\n","Epoch 51/90 Iteration 4820| Training loss: 1.2506\n","Epoch 51/90 Iteration 4830| Training loss: 1.2368\n","Epoch 51/90 Iteration 4840| Training loss: 1.2606\n","Epoch 51/90 Iteration 4850| Training loss: 1.2229\n","Epoch 51/90 Iteration 4860| Training loss: 1.2545\n","Epoch 51/90 Iteration 4870| Training loss: 1.2491\n","Epoch 51/90 Iteration 4880| Training loss: 1.2816\n","Epoch 51/90 Iteration 4890| Training loss: 1.2125\n","Epoch 52/90 Iteration 4900| Training loss: 1.2296\n","Epoch 52/90 Iteration 4910| Training loss: 1.2347\n","Epoch 52/90 Iteration 4920| Training loss: 1.2155\n","Epoch 52/90 Iteration 4930| Training loss: 1.2343\n","Epoch 52/90 Iteration 4940| Training loss: 1.2025\n","Epoch 52/90 Iteration 4950| Training loss: 1.2179\n","Epoch 52/90 Iteration 4960| Training loss: 1.2411\n","Epoch 52/90 Iteration 4970| Training loss: 1.2124\n","Epoch 52/90 Iteration 4980| Training loss: 1.2680\n","Epoch 52/90 Iteration 4990| Training loss: 1.2242\n","Epoch 53/90 Iteration 5000| Training loss: 1.2001\n","Epoch 53/90 Iteration 5010| Training loss: 1.2354\n","Epoch 53/90 Iteration 5020| Training loss: 1.1919\n","Epoch 53/90 Iteration 5030| Training loss: 1.2330\n","Epoch 53/90 Iteration 5040| Training loss: 1.2103\n","Epoch 53/90 Iteration 5050| Training loss: 1.2206\n","Epoch 53/90 Iteration 5060| Training loss: 1.2142\n","Epoch 53/90 Iteration 5070| Training loss: 1.2143\n","Epoch 53/90 Iteration 5080| Training loss: 1.2058\n","Epoch 54/90 Iteration 5090| Training loss: 1.2342\n","Epoch 54/90 Iteration 5100| Training loss: 1.2360\n","Epoch 54/90 Iteration 5110| Training loss: 1.2051\n","Epoch 54/90 Iteration 5120| Training loss: 1.2687\n","Epoch 54/90 Iteration 5130| Training loss: 1.1757\n","Epoch 54/90 Iteration 5140| Training loss: 1.2339\n","Epoch 54/90 Iteration 5150| Training loss: 1.2496\n","Epoch 54/90 Iteration 5160| Training loss: 1.2454\n","Epoch 54/90 Iteration 5170| Training loss: 1.2906\n","Epoch 54/90 Iteration 5180| Training loss: 1.2582\n","Epoch 55/90 Iteration 5190| Training loss: 1.2663\n","Epoch 55/90 Iteration 5200| Training loss: 1.2197\n","Epoch 55/90 Iteration 5210| Training loss: 1.2116\n","Epoch 55/90 Iteration 5220| Training loss: 1.1905\n","Epoch 55/90 Iteration 5230| Training loss: 1.1850\n","Epoch 55/90 Iteration 5240| Training loss: 1.2215\n","Epoch 55/90 Iteration 5250| Training loss: 1.1940\n","Epoch 55/90 Iteration 5260| Training loss: 1.2307\n","Epoch 55/90 Iteration 5270| Training loss: 1.1941\n","Epoch 55/90 Iteration 5280| Training loss: 1.2146\n","Epoch 56/90 Iteration 5290| Training loss: 1.1986\n","Epoch 56/90 Iteration 5300| Training loss: 1.2292\n","Epoch 56/90 Iteration 5310| Training loss: 1.2243\n","Epoch 56/90 Iteration 5320| Training loss: 1.2536\n","Epoch 56/90 Iteration 5330| Training loss: 1.2121\n","Epoch 56/90 Iteration 5340| Training loss: 1.2436\n","Epoch 56/90 Iteration 5350| Training loss: 1.2546\n","Epoch 56/90 Iteration 5360| Training loss: 1.2589\n","Epoch 56/90 Iteration 5370| Training loss: 1.2066\n","Epoch 57/90 Iteration 5380| Training loss: 1.2331\n","Epoch 57/90 Iteration 5390| Training loss: 1.2271\n","Epoch 57/90 Iteration 5400| Training loss: 1.2011\n","Epoch 57/90 Iteration 5410| Training loss: 1.2302\n","Epoch 57/90 Iteration 5420| Training loss: 1.2106\n","Epoch 57/90 Iteration 5430| Training loss: 1.2206\n","Epoch 57/90 Iteration 5440| Training loss: 1.2290\n","Epoch 57/90 Iteration 5450| Training loss: 1.1975\n","Epoch 57/90 Iteration 5460| Training loss: 1.2620\n","Epoch 57/90 Iteration 5470| Training loss: 1.2206\n","Epoch 58/90 Iteration 5480| Training loss: 1.2102\n","Epoch 58/90 Iteration 5490| Training loss: 1.2310\n","Epoch 58/90 Iteration 5500| Training loss: 1.1976\n","Epoch 58/90 Iteration 5510| Training loss: 1.2317\n","Epoch 58/90 Iteration 5520| Training loss: 1.2210\n","Epoch 58/90 Iteration 5530| Training loss: 1.2089\n","Epoch 58/90 Iteration 5540| Training loss: 1.2024\n","Epoch 58/90 Iteration 5550| Training loss: 1.2070\n","Epoch 58/90 Iteration 5560| Training loss: 1.2065\n","Epoch 59/90 Iteration 5570| Training loss: 1.2226\n","Epoch 59/90 Iteration 5580| Training loss: 1.2299\n","Epoch 59/90 Iteration 5590| Training loss: 1.2032\n","Epoch 59/90 Iteration 5600| Training loss: 1.2627\n","Epoch 59/90 Iteration 5610| Training loss: 1.1734\n","Epoch 59/90 Iteration 5620| Training loss: 1.2131\n","Epoch 59/90 Iteration 5630| Training loss: 1.2329\n","Epoch 59/90 Iteration 5640| Training loss: 1.2231\n","Epoch 59/90 Iteration 5650| Training loss: 1.2932\n","Epoch 59/90 Iteration 5660| Training loss: 1.2403\n","Epoch 60/90 Iteration 5670| Training loss: 1.2765\n","Epoch 60/90 Iteration 5680| Training loss: 1.2110\n","Epoch 60/90 Iteration 5690| Training loss: 1.2192\n","Epoch 60/90 Iteration 5700| Training loss: 1.1817\n","Epoch 60/90 Iteration 5710| Training loss: 1.1795\n","Epoch 60/90 Iteration 5720| Training loss: 1.2024\n","Epoch 60/90 Iteration 5730| Training loss: 1.1991\n","Epoch 60/90 Iteration 5740| Training loss: 1.2325\n","Epoch 60/90 Iteration 5750| Training loss: 1.2009\n","Epoch 60/90 Iteration 5760| Training loss: 1.1985\n","Epoch 61/90 Iteration 5770| Training loss: 1.1894\n","Epoch 61/90 Iteration 5780| Training loss: 1.2247\n","Epoch 61/90 Iteration 5790| Training loss: 1.2171\n","Epoch 61/90 Iteration 5800| Training loss: 1.2338\n","Epoch 61/90 Iteration 5810| Training loss: 1.2053\n","Epoch 61/90 Iteration 5820| Training loss: 1.2380\n","Epoch 61/90 Iteration 5830| Training loss: 1.2408\n","Epoch 61/90 Iteration 5840| Training loss: 1.2692\n","Epoch 61/90 Iteration 5850| Training loss: 1.2085\n","Epoch 62/90 Iteration 5860| Training loss: 1.2168\n","Epoch 62/90 Iteration 5870| Training loss: 1.2200\n","Epoch 62/90 Iteration 5880| Training loss: 1.1948\n","Epoch 62/90 Iteration 5890| Training loss: 1.2265\n","Epoch 62/90 Iteration 5900| Training loss: 1.1808\n","Epoch 62/90 Iteration 5910| Training loss: 1.2034\n","Epoch 62/90 Iteration 5920| Training loss: 1.2307\n","Epoch 62/90 Iteration 5930| Training loss: 1.1976\n","Epoch 62/90 Iteration 5940| Training loss: 1.2606\n","Epoch 62/90 Iteration 5950| Training loss: 1.2151\n","Epoch 63/90 Iteration 5960| Training loss: 1.2007\n","Epoch 63/90 Iteration 5970| Training loss: 1.2195\n","Epoch 63/90 Iteration 5980| Training loss: 1.2096\n","Epoch 63/90 Iteration 5990| Training loss: 1.2241\n","Epoch 63/90 Iteration 6000| Training loss: 1.1972\n","Epoch 63/90 Iteration 6010| Training loss: 1.2034\n","Epoch 63/90 Iteration 6020| Training loss: 1.1989\n","Epoch 63/90 Iteration 6030| Training loss: 1.2081\n","Epoch 63/90 Iteration 6040| Training loss: 1.2041\n","Epoch 64/90 Iteration 6050| Training loss: 1.2308\n","Epoch 64/90 Iteration 6060| Training loss: 1.2221\n","Epoch 64/90 Iteration 6070| Training loss: 1.2042\n","Epoch 64/90 Iteration 6080| Training loss: 1.2595\n","Epoch 64/90 Iteration 6090| Training loss: 1.1647\n","Epoch 64/90 Iteration 6100| Training loss: 1.2078\n","Epoch 64/90 Iteration 6110| Training loss: 1.2316\n","Epoch 64/90 Iteration 6120| Training loss: 1.2348\n","Epoch 64/90 Iteration 6130| Training loss: 1.2791\n","Epoch 64/90 Iteration 6140| Training loss: 1.2454\n","Epoch 65/90 Iteration 6150| Training loss: 1.2614\n","Epoch 65/90 Iteration 6160| Training loss: 1.2173\n","Epoch 65/90 Iteration 6170| Training loss: 1.2086\n","Epoch 65/90 Iteration 6180| Training loss: 1.1665\n","Epoch 65/90 Iteration 6190| Training loss: 1.1713\n","Epoch 65/90 Iteration 6200| Training loss: 1.2104\n","Epoch 65/90 Iteration 6210| Training loss: 1.1956\n","Epoch 65/90 Iteration 6220| Training loss: 1.2285\n","Epoch 65/90 Iteration 6230| Training loss: 1.1879\n","Epoch 65/90 Iteration 6240| Training loss: 1.1961\n","Epoch 66/90 Iteration 6250| Training loss: 1.1899\n","Epoch 66/90 Iteration 6260| Training loss: 1.2093\n","Epoch 66/90 Iteration 6270| Training loss: 1.2174\n","Epoch 66/90 Iteration 6280| Training loss: 1.2493\n","Epoch 66/90 Iteration 6290| Training loss: 1.2056\n","Epoch 66/90 Iteration 6300| Training loss: 1.2320\n","Epoch 66/90 Iteration 6310| Training loss: 1.2462\n","Epoch 66/90 Iteration 6320| Training loss: 1.2460\n","Epoch 66/90 Iteration 6330| Training loss: 1.1938\n","Epoch 67/90 Iteration 6340| Training loss: 1.2031\n","Epoch 67/90 Iteration 6350| Training loss: 1.2303\n","Epoch 67/90 Iteration 6360| Training loss: 1.2020\n","Epoch 67/90 Iteration 6370| Training loss: 1.2192\n","Epoch 67/90 Iteration 6380| Training loss: 1.1817\n","Epoch 67/90 Iteration 6390| Training loss: 1.2194\n","Epoch 67/90 Iteration 6400| Training loss: 1.2201\n","Epoch 67/90 Iteration 6410| Training loss: 1.1878\n","Epoch 67/90 Iteration 6420| Training loss: 1.2517\n","Epoch 67/90 Iteration 6430| Training loss: 1.2156\n","Epoch 68/90 Iteration 6440| Training loss: 1.1909\n","Epoch 68/90 Iteration 6450| Training loss: 1.2070\n","Epoch 68/90 Iteration 6460| Training loss: 1.1929\n","Epoch 68/90 Iteration 6470| Training loss: 1.2059\n","Epoch 68/90 Iteration 6480| Training loss: 1.1974\n","Epoch 68/90 Iteration 6490| Training loss: 1.1925\n","Epoch 68/90 Iteration 6500| Training loss: 1.1813\n","Epoch 68/90 Iteration 6510| Training loss: 1.1912\n","Epoch 68/90 Iteration 6520| Training loss: 1.1953\n","Epoch 69/90 Iteration 6530| Training loss: 1.2299\n","Epoch 69/90 Iteration 6540| Training loss: 1.2230\n","Epoch 69/90 Iteration 6550| Training loss: 1.1817\n","Epoch 69/90 Iteration 6560| Training loss: 1.2672\n","Epoch 69/90 Iteration 6570| Training loss: 1.1695\n","Epoch 69/90 Iteration 6580| Training loss: 1.2081\n","Epoch 69/90 Iteration 6590| Training loss: 1.2300\n","Epoch 69/90 Iteration 6600| Training loss: 1.2132\n","Epoch 69/90 Iteration 6610| Training loss: 1.2707\n","Epoch 69/90 Iteration 6620| Training loss: 1.2462\n","Epoch 70/90 Iteration 6630| Training loss: 1.2493\n","Epoch 70/90 Iteration 6640| Training loss: 1.2124\n","Epoch 70/90 Iteration 6650| Training loss: 1.1971\n","Epoch 70/90 Iteration 6660| Training loss: 1.1632\n","Epoch 70/90 Iteration 6670| Training loss: 1.1683\n","Epoch 70/90 Iteration 6680| Training loss: 1.1941\n","Epoch 70/90 Iteration 6690| Training loss: 1.1941\n","Epoch 70/90 Iteration 6700| Training loss: 1.2191\n","Epoch 70/90 Iteration 6710| Training loss: 1.1916\n","Epoch 70/90 Iteration 6720| Training loss: 1.1833\n","Epoch 71/90 Iteration 6730| Training loss: 1.1766\n","Epoch 71/90 Iteration 6740| Training loss: 1.2080\n","Epoch 71/90 Iteration 6750| Training loss: 1.1922\n","Epoch 71/90 Iteration 6760| Training loss: 1.2393\n","Epoch 71/90 Iteration 6770| Training loss: 1.1969\n","Epoch 71/90 Iteration 6780| Training loss: 1.2284\n","Epoch 71/90 Iteration 6790| Training loss: 1.2408\n","Epoch 71/90 Iteration 6800| Training loss: 1.2389\n","Epoch 71/90 Iteration 6810| Training loss: 1.1825\n","Epoch 72/90 Iteration 6820| Training loss: 1.2152\n","Epoch 72/90 Iteration 6830| Training loss: 1.2158\n","Epoch 72/90 Iteration 6840| Training loss: 1.1952\n","Epoch 72/90 Iteration 6850| Training loss: 1.2194\n","Epoch 72/90 Iteration 6860| Training loss: 1.1815\n","Epoch 72/90 Iteration 6870| Training loss: 1.2058\n","Epoch 72/90 Iteration 6880| Training loss: 1.2054\n","Epoch 72/90 Iteration 6890| Training loss: 1.1848\n","Epoch 72/90 Iteration 6900| Training loss: 1.2410\n","Epoch 72/90 Iteration 6910| Training loss: 1.1954\n","Epoch 73/90 Iteration 6920| Training loss: 1.1849\n","Epoch 73/90 Iteration 6930| Training loss: 1.2165\n","Epoch 73/90 Iteration 6940| Training loss: 1.1818\n","Epoch 73/90 Iteration 6950| Training loss: 1.1982\n","Epoch 73/90 Iteration 6960| Training loss: 1.1844\n","Epoch 73/90 Iteration 6970| Training loss: 1.2030\n","Epoch 73/90 Iteration 6980| Training loss: 1.1973\n","Epoch 73/90 Iteration 6990| Training loss: 1.1939\n","Epoch 73/90 Iteration 7000| Training loss: 1.1953\n","Epoch 74/90 Iteration 7010| Training loss: 1.2151\n","Epoch 74/90 Iteration 7020| Training loss: 1.2094\n","Epoch 74/90 Iteration 7030| Training loss: 1.1805\n","Epoch 74/90 Iteration 7040| Training loss: 1.2491\n","Epoch 74/90 Iteration 7050| Training loss: 1.1607\n","Epoch 74/90 Iteration 7060| Training loss: 1.2155\n","Epoch 74/90 Iteration 7070| Training loss: 1.2395\n","Epoch 74/90 Iteration 7080| Training loss: 1.2171\n","Epoch 74/90 Iteration 7090| Training loss: 1.2782\n","Epoch 74/90 Iteration 7100| Training loss: 1.2381\n","Epoch 75/90 Iteration 7110| Training loss: 1.2635\n","Epoch 75/90 Iteration 7120| Training loss: 1.2009\n","Epoch 75/90 Iteration 7130| Training loss: 1.1891\n","Epoch 75/90 Iteration 7140| Training loss: 1.1588\n","Epoch 75/90 Iteration 7150| Training loss: 1.1486\n","Epoch 75/90 Iteration 7160| Training loss: 1.1974\n","Epoch 75/90 Iteration 7170| Training loss: 1.1909\n","Epoch 75/90 Iteration 7180| Training loss: 1.2141\n","Epoch 75/90 Iteration 7190| Training loss: 1.1823\n","Epoch 75/90 Iteration 7200| Training loss: 1.1932\n","Epoch 76/90 Iteration 7210| Training loss: 1.1609\n","Epoch 76/90 Iteration 7220| Training loss: 1.2022\n","Epoch 76/90 Iteration 7230| Training loss: 1.1869\n","Epoch 76/90 Iteration 7240| Training loss: 1.2295\n","Epoch 76/90 Iteration 7250| Training loss: 1.1900\n","Epoch 76/90 Iteration 7260| Training loss: 1.2216\n","Epoch 76/90 Iteration 7270| Training loss: 1.2291\n","Epoch 76/90 Iteration 7280| Training loss: 1.2496\n","Epoch 76/90 Iteration 7290| Training loss: 1.1779\n","Epoch 77/90 Iteration 7300| Training loss: 1.1973\n","Epoch 77/90 Iteration 7310| Training loss: 1.2041\n","Epoch 77/90 Iteration 7320| Training loss: 1.1956\n","Epoch 77/90 Iteration 7330| Training loss: 1.2051\n","Epoch 77/90 Iteration 7340| Training loss: 1.1783\n","Epoch 77/90 Iteration 7350| Training loss: 1.1994\n","Epoch 77/90 Iteration 7360| Training loss: 1.2121\n","Epoch 77/90 Iteration 7370| Training loss: 1.1814\n","Epoch 77/90 Iteration 7380| Training loss: 1.2453\n","Epoch 77/90 Iteration 7390| Training loss: 1.2009\n","Epoch 78/90 Iteration 7400| Training loss: 1.1766\n","Epoch 78/90 Iteration 7410| Training loss: 1.2209\n","Epoch 78/90 Iteration 7420| Training loss: 1.1851\n","Epoch 78/90 Iteration 7430| Training loss: 1.2166\n","Epoch 78/90 Iteration 7440| Training loss: 1.1829\n","Epoch 78/90 Iteration 7450| Training loss: 1.1956\n","Epoch 78/90 Iteration 7460| Training loss: 1.1775\n","Epoch 78/90 Iteration 7470| Training loss: 1.1991\n","Epoch 78/90 Iteration 7480| Training loss: 1.1899\n","Epoch 79/90 Iteration 7490| Training loss: 1.2053\n","Epoch 79/90 Iteration 7500| Training loss: 1.2162\n","Epoch 79/90 Iteration 7510| Training loss: 1.1794\n","Epoch 79/90 Iteration 7520| Training loss: 1.2376\n","Epoch 79/90 Iteration 7530| Training loss: 1.1486\n","Epoch 79/90 Iteration 7540| Training loss: 1.2036\n","Epoch 79/90 Iteration 7550| Training loss: 1.2266\n","Epoch 79/90 Iteration 7560| Training loss: 1.2069\n","Epoch 79/90 Iteration 7570| Training loss: 1.2547\n","Epoch 79/90 Iteration 7580| Training loss: 1.2269\n","Epoch 80/90 Iteration 7590| Training loss: 1.2417\n","Epoch 80/90 Iteration 7600| Training loss: 1.1914\n","Epoch 80/90 Iteration 7610| Training loss: 1.1843\n","Epoch 80/90 Iteration 7620| Training loss: 1.1506\n","Epoch 80/90 Iteration 7630| Training loss: 1.1532\n","Epoch 80/90 Iteration 7640| Training loss: 1.1866\n","Epoch 80/90 Iteration 7650| Training loss: 1.1769\n","Epoch 80/90 Iteration 7660| Training loss: 1.2184\n","Epoch 80/90 Iteration 7670| Training loss: 1.1691\n","Epoch 80/90 Iteration 7680| Training loss: 1.1744\n","Epoch 81/90 Iteration 7690| Training loss: 1.1688\n","Epoch 81/90 Iteration 7700| Training loss: 1.2020\n","Epoch 81/90 Iteration 7710| Training loss: 1.1851\n","Epoch 81/90 Iteration 7720| Training loss: 1.2161\n","Epoch 81/90 Iteration 7730| Training loss: 1.1862\n","Epoch 81/90 Iteration 7740| Training loss: 1.2121\n","Epoch 81/90 Iteration 7750| Training loss: 1.2222\n","Epoch 81/90 Iteration 7760| Training loss: 1.2410\n","Epoch 81/90 Iteration 7770| Training loss: 1.1769\n","Epoch 82/90 Iteration 7780| Training loss: 1.2065\n","Epoch 82/90 Iteration 7790| Training loss: 1.2038\n","Epoch 82/90 Iteration 7800| Training loss: 1.1764\n","Epoch 82/90 Iteration 7810| Training loss: 1.2068\n","Epoch 82/90 Iteration 7820| Training loss: 1.1666\n","Epoch 82/90 Iteration 7830| Training loss: 1.1942\n","Epoch 82/90 Iteration 7840| Training loss: 1.2093\n","Epoch 82/90 Iteration 7850| Training loss: 1.1695\n","Epoch 82/90 Iteration 7860| Training loss: 1.2360\n","Epoch 82/90 Iteration 7870| Training loss: 1.1971\n","Epoch 83/90 Iteration 7880| Training loss: 1.1733\n","Epoch 83/90 Iteration 7890| Training loss: 1.2056\n","Epoch 83/90 Iteration 7900| Training loss: 1.1756\n","Epoch 83/90 Iteration 7910| Training loss: 1.2057\n","Epoch 83/90 Iteration 7920| Training loss: 1.1801\n","Epoch 83/90 Iteration 7930| Training loss: 1.1818\n","Epoch 83/90 Iteration 7940| Training loss: 1.1797\n","Epoch 83/90 Iteration 7950| Training loss: 1.1856\n","Epoch 83/90 Iteration 7960| Training loss: 1.1722\n","Epoch 84/90 Iteration 7970| Training loss: 1.2090\n","Epoch 84/90 Iteration 7980| Training loss: 1.2081\n","Epoch 84/90 Iteration 7990| Training loss: 1.1749\n","Epoch 84/90 Iteration 8000| Training loss: 1.2357\n","Epoch 84/90 Iteration 8010| Training loss: 1.1558\n","Epoch 84/90 Iteration 8020| Training loss: 1.2134\n","Epoch 84/90 Iteration 8030| Training loss: 1.2222\n","Epoch 84/90 Iteration 8040| Training loss: 1.1930\n","Epoch 84/90 Iteration 8050| Training loss: 1.2655\n","Epoch 84/90 Iteration 8060| Training loss: 1.2161\n","Epoch 85/90 Iteration 8070| Training loss: 1.2448\n","Epoch 85/90 Iteration 8080| Training loss: 1.1883\n","Epoch 85/90 Iteration 8090| Training loss: 1.1766\n","Epoch 85/90 Iteration 8100| Training loss: 1.1407\n","Epoch 85/90 Iteration 8110| Training loss: 1.1421\n","Epoch 85/90 Iteration 8120| Training loss: 1.1853\n","Epoch 85/90 Iteration 8130| Training loss: 1.1729\n","Epoch 85/90 Iteration 8140| Training loss: 1.1999\n","Epoch 85/90 Iteration 8150| Training loss: 1.1631\n","Epoch 85/90 Iteration 8160| Training loss: 1.1785\n","Epoch 86/90 Iteration 8170| Training loss: 1.1649\n","Epoch 86/90 Iteration 8180| Training loss: 1.1920\n","Epoch 86/90 Iteration 8190| Training loss: 1.1864\n","Epoch 86/90 Iteration 8200| Training loss: 1.2038\n","Epoch 86/90 Iteration 8210| Training loss: 1.1793\n","Epoch 86/90 Iteration 8220| Training loss: 1.2126\n","Epoch 86/90 Iteration 8230| Training loss: 1.2328\n","Epoch 86/90 Iteration 8240| Training loss: 1.2285\n","Epoch 86/90 Iteration 8250| Training loss: 1.1748\n","Epoch 87/90 Iteration 8260| Training loss: 1.2038\n","Epoch 87/90 Iteration 8270| Training loss: 1.1989\n","Epoch 87/90 Iteration 8280| Training loss: 1.1939\n","Epoch 87/90 Iteration 8290| Training loss: 1.1970\n","Epoch 87/90 Iteration 8300| Training loss: 1.1694\n","Epoch 87/90 Iteration 8310| Training loss: 1.1887\n","Epoch 87/90 Iteration 8320| Training loss: 1.2107\n","Epoch 87/90 Iteration 8330| Training loss: 1.1548\n","Epoch 87/90 Iteration 8340| Training loss: 1.2251\n","Epoch 87/90 Iteration 8350| Training loss: 1.1884\n","Epoch 88/90 Iteration 8360| Training loss: 1.1795\n","Epoch 88/90 Iteration 8370| Training loss: 1.1973\n","Epoch 88/90 Iteration 8380| Training loss: 1.1589\n","Epoch 88/90 Iteration 8390| Training loss: 1.1885\n","Epoch 88/90 Iteration 8400| Training loss: 1.1899\n","Epoch 88/90 Iteration 8410| Training loss: 1.1826\n","Epoch 88/90 Iteration 8420| Training loss: 1.1675\n","Epoch 88/90 Iteration 8430| Training loss: 1.1895\n","Epoch 88/90 Iteration 8440| Training loss: 1.1660\n","Epoch 89/90 Iteration 8450| Training loss: 1.1947\n","Epoch 89/90 Iteration 8460| Training loss: 1.2000\n","Epoch 89/90 Iteration 8470| Training loss: 1.1627\n","Epoch 89/90 Iteration 8480| Training loss: 1.2284\n","Epoch 89/90 Iteration 8490| Training loss: 1.1541\n","Epoch 89/90 Iteration 8500| Training loss: 1.1911\n","Epoch 89/90 Iteration 8510| Training loss: 1.2170\n","Epoch 89/90 Iteration 8520| Training loss: 1.1950\n","Epoch 89/90 Iteration 8530| Training loss: 1.2480\n","Epoch 89/90 Iteration 8540| Training loss: 1.2189\n","Epoch 90/90 Iteration 8550| Training loss: 1.2340\n","Epoch 90/90 Iteration 8560| Training loss: 1.1810\n","Epoch 90/90 Iteration 8570| Training loss: 1.1680\n","Epoch 90/90 Iteration 8580| Training loss: 1.1546\n","Epoch 90/90 Iteration 8590| Training loss: 1.1418\n","Epoch 90/90 Iteration 8600| Training loss: 1.1748\n","Epoch 90/90 Iteration 8610| Training loss: 1.1634\n","Epoch 90/90 Iteration 8620| Training loss: 1.2020\n","Epoch 90/90 Iteration 8630| Training loss: 1.1601\n","Epoch 90/90 Iteration 8640| Training loss: 1.1630\n","<< lstm_outputs >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 250), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/model-100/language_modeling.ckpt\n","['u', ' ', 'w', 'h', 'i', 'c', 'h']\n","Word generated:  u which\n","Looking for:  DT\n","Word found:  NN\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  DT\n","Word found:  VBZ\n","Failed\n","['a']\n","Word generated:  a\n","Looking for:  DT\n","Word found:  DT\n","['p', 'r', 'i', 's', 'o', 'n', 'e', 'r', 's']\n","Word generated:  prisoners\n","Looking for:  NN\n","Word found:  NNS\n","Failed\n","['s', 'e', 'e']\n","Word generated:  see\n","Looking for:  NN\n","Word found:  VB\n","Failed\n","['a', 't']\n","Word generated:  at\n","Looking for:  NN\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  NN\n","Word found:  DT\n","Failed\n","['p', 'a', 't', 'h']\n","Word generated:  path\n","Looking for:  NN\n","Word found:  NN\n","['o', 'f']\n","Word generated:  of\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['a', 'w', 'a', 'r', 'e', 'n', 'e', 's', 's']\n","Word generated:  awareness\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['i']\n","Word generated:  i\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['a', 'm']\n","Word generated:  am\n","Looking for:  VBZ\n","Word found:  VBP\n","Failed\n","['c', 'o', 'm', 'e']\n","Word generated:  come\n","Looking for:  VBZ\n","Word found:  VB\n","Failed\n","['f', 'r', 'o', 'm']\n","Word generated:  from\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['s', 'h', 'a', 'p', 'e', 'd']\n","Word generated:  shaped\n","Looking for:  VBZ\n","Word found:  VBD\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  VBZ\n","Word found:  DT\n","Failed\n","['p', 'e', 'r', 's', 'o', 'n']\n","Word generated:  person\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['w', 'h', 'o']\n","Word generated:  who\n","Looking for:  VBZ\n","Word found:  WP\n","Failed\n","['d', 'i', 'd', 'n', 't']\n","Word generated:  didnt\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['a', 'r', 'e']\n","Word generated:  are\n","Looking for:  VBZ\n","Word found:  VBP\n","Failed\n","['t', 'r', 'y', 'i', 'n', 'g']\n","Word generated:  trying\n","Looking for:  VBZ\n","Word found:  VBG\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  VBZ\n","Word found:  TO\n","Failed\n","['b', 'e']\n","Word generated:  be\n","Looking for:  VBZ\n","Word found:  VB\n","Failed\n","['c', 'l', 'o', 's', 'e', 'd']\n","Word generated:  closed\n","Looking for:  VBZ\n","Word found:  VBD\n","Failed\n","['a', 'n', 'd']\n","Word generated:  and\n","Looking for:  VBZ\n","Word found:  CC\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['s', 'p', 'i', 'r', 'i', 't', 'u', 'a', 'l']\n","Word generated:  spiritual\n","Looking for:  VBZ\n","Word found:  JJ\n","Failed\n","['t', 'h', 'a', 'n', 'k', 's']\n","Word generated:  thanks\n","Looking for:  VBZ\n","Word found:  NNS\n","Failed\n","['f', 'o', 'r']\n","Word generated:  for\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['s', 'p', 'i', 'r', 'i', 't', 'u', 'a', 'l', 'i', 't', 'y']\n","Word generated:  spirituality\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","[]\n","Word generated:  \n","[' ', 'y', 'o', 'u', 't', 'u', 'b']\n","Word generated:   youtub\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['y', 'o', 'u']\n","Word generated:  you\n","Looking for:  VBZ\n","Word found:  PRP\n","Failed\n","['w', 'a', 'n', 't']\n","Word generated:  want\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  VBZ\n","Word found:  TO\n","Failed\n","['s', 'h', 'o', 'r', 't']\n","Word generated:  short\n","Looking for:  VBZ\n","Word found:  JJ\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  VBZ\n","Word found:  TO\n","Failed\n","['b', 'e']\n","Word generated:  be\n","Looking for:  VBZ\n","Word found:  VB\n","Failed\n","['t', 'a', 'k', 'e']\n","Word generated:  take\n","Looking for:  VBZ\n","Word found:  VB\n","Failed\n","['t', 'h', 'e', 'm']\n","Word generated:  them\n","Looking for:  VBZ\n","Word found:  PRP\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  VBZ\n","Word found:  DT\n","Failed\n","['w', 'o', 'r', 'l', 'd']\n","Word generated:  world\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['t', 'h', 'a', 't']\n","Word generated:  that\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  VBZ\n","Word found:  TO\n","Failed\n","['d', 'o']\n","Word generated:  do\n","Looking for:  VBZ\n","Word found:  VB\n","Failed\n","['t', 'h', 'e', 'r', 'e']\n","Word generated:  there\n","Looking for:  VBZ\n","Word found:  RB\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  VBZ\n","Word found:  VBZ\n","['n', 'o']\n","Word generated:  no\n","Looking for:  JJ\n","Word found:  DT\n","Failed\n","['s', 'e', 'r', 'i', 'o', 'u', 's']\n","Word generated:  serious\n","Looking for:  JJ\n","Word found:  JJ\n","['i', 'n']\n","Word generated:  in\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  CC\n","Word found:  DT\n","Failed\n","['p', 'r', 'o', 'j', 'e', 'c', 't']\n","Word generated:  project\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['w', 'o', 'r', 'k']\n","Word generated:  work\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['a', 'n', 'd']\n","Word generated:  and\n","Looking for:  CC\n","Word found:  CC\n","['t', 'h', 'a', 't']\n","Word generated:  that\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['w', 'i', 'l', 'l']\n","Word generated:  will\n","Looking for:  RB\n","Word found:  MD\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['t', 'h', 'e', 'i', 'r']\n","Word generated:  their\n","Looking for:  RB\n","Word found:  PRP$\n","Failed\n","['o', 'w', 'n']\n","Word generated:  own\n","Looking for:  RB\n","Word found:  JJ\n","Failed\n","['s', 't', 'o', 'r', 'i', 'e', 's']\n","Word generated:  stories\n","Looking for:  RB\n","Word found:  NNS\n","Failed\n","['t', 'r', 'y']\n","Word generated:  try\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['i', 't']\n","Word generated:  it\n","Looking for:  RB\n","Word found:  PRP\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  RB\n","Word found:  DT\n","Failed\n","['f', 'e', 't', 'm', 'o', 'n', 't']\n","Word generated:  fetmont\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['w', 'h', 'o']\n","Word generated:  who\n","Looking for:  RB\n","Word found:  WP\n","Failed\n","['w', 'a', 's']\n","Word generated:  was\n","Looking for:  RB\n","Word found:  VBD\n","Failed\n","['a', 'l', 's', 'o']\n","Word generated:  also\n","Looking for:  RB\n","Word found:  RB\n","['i', 'n']\n","Word generated:  in\n","Looking for:  DT\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  DT\n","Word found:  DT\n","['m', 'o', 's', 't']\n","Word generated:  most\n","Looking for:  NN\n","Word found:  JJS\n","Failed\n","['a', 'n', 'd']\n","Word generated:  and\n","Looking for:  NN\n","Word found:  CC\n","Failed\n","['i']\n","Word generated:  i\n","Looking for:  NN\n","Word found:  NN\n","['d', 'o', 'n', 't']\n","Word generated:  dont\n","Looking for:  IN\n","Word found:  NN\n","Failed\n","['w', 'a', 'n', 't']\n","Word generated:  want\n","Looking for:  IN\n","Word found:  NN\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  IN\n","Word found:  TO\n","Failed\n","['b', 'e']\n","Word generated:  be\n","Looking for:  IN\n","Word found:  VB\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  IN\n","Word found:  DT\n","Failed\n","['s', 'a', 'm', 'e']\n","Word generated:  same\n","Looking for:  IN\n","Word found:  JJ\n","Failed\n","['i']\n","Word generated:  i\n","Looking for:  IN\n","Word found:  NN\n","Failed\n","['s', 't', 'a', 't', 'e']\n","Word generated:  state\n","Looking for:  IN\n","Word found:  NN\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  IN\n","Word found:  IN\n","['s', 'e', 'n', 's', 'e']\n","Word generated:  sense\n","Looking for:  NN\n","Word found:  NN\n","a path is serious and also the i in sense \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA18VdmcuVYOIAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"L2GZuFODk1NN","colab_type":"code","outputId":"9636ee0b-5f2f-43cc-c248-55ae180b374e","executionInfo":{"status":"ok","timestamp":1556500115913,"user_tz":300,"elapsed":2861,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["%reset"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n","Nothing done.\n"],"name":"stdout"}]},{"metadata":{"id":"He6g9R59jLnZ","colab_type":"code","outputId":"a46e6472-368c-4e66-aab2-aaf149c59801","executionInfo":{"status":"ok","timestamp":1556500120519,"user_tz":300,"elapsed":2937,"user":{"displayName":"GRANT DAKOVICH","photoUrl":"","userId":"12992330660596314814"}},"colab":{"base_uri":"https://localhost:8080/","height":8265}},"cell_type":"code","source":["import numpy as np\n","import torch\n","import pandas as pd\n","import csv\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","import sys\n","import random\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","batch_size = 64\n","num_steps = 100\n","\n","#print(rnn.sample(ckpt_dir='/content/drive/My Drive/posmodel-100/', output_length=35, pospriors = ['DT', 'NN', 'VBZ', 'JJ', 'CC', 'RB', 'DT', 'NN', 'IN', 'NN']))\n","priors_in = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/out_pos.csv\") \n","_pospriors = priors_in['pos_sequence'].tolist()\n","for i in range(len(_pospriors)):\n","  print(_pospriors[i])\n","##########################################################################\n","\n","batches = []\n","\n","# Function for splitting data\n","def split_data(sequence, batch_size, num_steps):\n","    total_length = batch_size * num_steps\n","    num_batches = int(len(sequence) / total_length)\n","    if num_batches*total_length + 1 > len(sequence):\n","        num_batches = num_batches - 1\n","    # Cut down character stream to length of a batch\n","    inputs = sequence[0: num_batches * total_length]\n","    output = sequence[1: num_batches * total_length + 1]\n","    # Split input & output:\n","    split_input = np.split(inputs, batch_size)\n","    split_output = np.split(output, batch_size)\n","    # Combine the batches\n","    inputs = np.stack(split_input)\n","    output = np.stack(split_output)\n","    return inputs, output\n","\n","\n","def create_batch_generator(data_x, data_y, num_steps):\n","    batch_size, total_length = data_x.shape\n","    num_batches = int(total_length/num_steps)\n","    for b in range(num_batches):\n","        yield (data_x[:, b*num_steps:(b+1)*num_steps],\n","               data_y[:, b*num_steps:(b+1)*num_steps])\n","\n","\n","def get_top_char(probas, char_size, top_n=5):\n","    p = np.squeeze(probas)\n","    p[np.argsort(p)[:-top_n]] = 0.0\n","    p = p / np.sum(p)\n","    ch_id = np.random.choice(char_size, 1, p=p)[0]\n","    return ch_id\n","\n","# Class for character level recurrent neural network\n","class CharRNN(object):\n","    # Constructor (note, the sampling parameter is for determining\n","    # what mode this object is in (training/sampling), and grad_clip\n","    # is for preventing exploding gradients\n","    def __init__(self, num_classes, batch_size=64,\n","                 num_steps=100, lstm_size = _lstm_size, #lstm_size=128,\n","                 num_layers = _num_layers, learning_rate = _learning_rate, #num_layers=1, learning_rate=0.001,\n","                 keep_prob = _keep_prob, grad_clip = _grad_clip, #keep_prob=0.5, grad_clip=5,\n","                 sampling=False):\n","        # Set variables to values given by parameters\n","        self.num_classes = num_classes\n","        self.batch_size = batch_size\n","        self.num_steps = num_steps\n","        self.lstm_size = lstm_size\n","        self.num_layers = num_layers\n","        self.learning_rate = learning_rate\n","        self.keep_prob = keep_prob\n","        self.grad_clip = grad_clip\n","        self.g = tf.Graph()\n","\n","        with self.g.as_default():\n","            tf.set_random_seed(123)\n","            self.build(sampling=sampling)  # builds x and y graphs of data\n","            self.init_op = tf.global_variables_initializer()\n","            self.saver = tf.train.Saver()\n","            \n","\n","    def build(self, sampling):\n","        if sampling:\n","            batch_size, num_steps = 1, 1\n","        else:\n","            batch_size = self.batch_size\n","            num_steps = self.num_steps\n","        tf_x = tf.placeholder(tf.int32,\n","                              shape=[batch_size, num_steps],\n","                              name='tf_x')\n","        tf_y = tf.placeholder(tf.int32,\n","                              shape=[batch_size, num_steps],\n","                              name='tf_y')\n","        tf_keepprob = tf.placeholder(tf.float32,\n","                                     name='tf_keepprob')\n","\n","        # One-hot encoding:\n","        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n","        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n","\n","        # Build the multi-layer RNN cells\n","        cells = tf.contrib.rnn.MultiRNNCell(\n","            [tf.contrib.rnn.DropoutWrapper(\n","                tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n","                output_keep_prob=tf_keepprob)\n","            for _ in range(self.num_layers)])\n","\n","        # Define the initial state\n","        self.initial_state = cells.zero_state(\n","            batch_size, tf.float32)\n","\n","        # Run each sequence step through the RNN\n","        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n","            cells, x_onehot,\n","            initial_state=self.initial_state)\n","        print('<< lstm_outputs >>', lstm_outputs)\n","        seq_output_reshaped = tf.reshape(\n","            lstm_outputs,\n","            shape=[-1, self.lstm_size],\n","            name='seq_output_reshaped')\n","\n","        logits = tf.layers.dense(\n","            inputs=seq_output_reshaped,\n","            units=self.num_classes,\n","            activation=None,\n","            name='logits')\n","        probas = tf.nn.softmax(\n","            logits,\n","            name='probabilities')\n","\n","        y_reshaped = tf.reshape(\n","            y_onehot,\n","            shape=[-1, self.num_classes],\n","            name='y_reshaped')\n","        cost = tf.reduce_mean(\n","            tf.nn.softmax_cross_entropy_with_logits(\n","                logits=logits,\n","                labels=y_reshaped),\n","            name='cost')\n","\n","        # Gradient clipping to avoid \"exploding gradients\"\n","        tvars = tf.trainable_variables()\n","        grads, _ = tf.clip_by_global_norm(\n","            tf.gradients(cost, tvars),\n","            self.grad_clip)\n","        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n","        train_op = optimizer.apply_gradients(\n","            zip(grads, tvars),\n","            name='train_op')\n","\n","\n","    def train(self, train_x, train_y,\n","        num_epochs, ckpt_dir='./model/'):\n","\n","        # Create the checkpoint directory\n","        # if it does not exists\n","        if not os.path.exists(ckpt_dir):\n","            os.mkdir(ckpt_dir)\n","\n","        with tf.Session(graph=self.g) as sess:\n","            sess.run(self.init_op)\n","            n_batches = int(train_x.shape[1] / self.num_steps)\n","            iterations = n_batches * num_epochs\n","            for epoch in range(num_epochs):\n","\n","                # Train network\n","                new_state = sess.run(self.initial_state)\n","                loss = 0\n","\n","                # Mini-batch generator\n","                minibatchgen = create_batch_generator(\n","                    train_x, train_y, self.num_steps)\n","\n","                for b, (batch_x, batch_y) in enumerate(minibatchgen, 1):\n","                    iteration = epoch * n_batches + b\n","                    feed = {'tf_x:0': batch_x,\n","                            'tf_y:0': batch_y,\n","                            'tf_keepprob:0': self.keep_prob,\n","                            self.initial_state: new_state}\n","                    batch_cost, _, new_state = sess.run(\n","                        ['cost:0', 'train_op',\n","                         self.final_state],\n","                        feed_dict=feed)\n","                    if iteration % 10 == 0:\n","                        print('Epoch %d/%d Iteration %d'\n","                              '| Training loss: %.4f' % (\n","                                  epoch + 1, num_epochs,\n","                                  iteration, batch_cost))\n","                        batches.append(batch_cost)\n","\n","                # Save the trained model\n","                self.saver.save(\n","                    sess, os.path.join(\n","                        ckpt_dir, 'language_modeling.ckpt'))\n","\n","\n","    def sample(self, output_length,\n","               ckpt_dir, pospriors = ['NN','NN','NN','NN','NN']):\n","        #print(\"hi\")\n","        temp = list(tweetChars)\n","        randFirstLetter = temp[random.randint(0,len(tweetChars) - 1)]\n","        \n","        observed_seq = [randFirstLetter]\n","        starter_seq = [randFirstLetter]\n","        with tf.Session(graph=self.g) as sess:\n","            self.saver.restore(\n","            sess,\n","            tf.train.latest_checkpoint(ckpt_dir))\n","            \n","            # 1: run the model using the starter sequence\n","            new_state = sess.run(self.initial_state)\n","            #print(new_state)\n","            for ch in starter_seq:\n","                x = np.zeros((1, 1))\n","                x[0, 0] = char2int[ch.lower()]\n","                feed = {'tf_x:0': x,\n","                        'tf_keepprob:0': 1.0,\n","                        self.initial_state: new_state}\n","                probas, new_state = sess.run(\n","                            ['probabilities:0', self.final_state],\n","                            feed_dict=feed)\n","            ch_id = get_top_char(probas, len(tweetChars))\n","            observed_seq.append(int2char[ch_id])\n","\n","            # 2: run the model using the updated observed_seq\n","            end_of_last_word = 0\n","            word_pointer = 0\n","            i = 2\n","            while i < output_length and word_pointer < len(pospriors):\n","                x[0,0] = ch_id\n","                feed = {'tf_x:0': x,\n","                'tf_keepprob:0': 1.0,\n","                        self.initial_state: new_state}\n","                probas, new_state = sess.run(\n","                                ['probabilities:0', self.final_state],\n","                                feed_dict=feed)\n","\n","                ch_id = get_top_char(probas, len(tweetChars))\n","                # if we are moving to a new word...\n","                if ((int2char[ch_id] == \" \" or int2char[ch_id] == \"\\n\" or i == (output_length - 1))):\n","                  observed_seq.append(int2char[ch_id])\n","                  print(observed_seq[end_of_last_word:i])\n","                  word = ''.join(observed_seq[end_of_last_word:i])\n","                  print(\"Word generated: \",word)\n","                  \n","                  \n","                  if (len(nltk.pos_tag(nltk.word_tokenize(word))) != 0):\n","                    #print(nltk.pos_tag(nltk.word_tokenize(word)))\n","                    print(\"Looking for: \",pospriors[word_pointer])\n","                    print(\"Word found: \",nltk.pos_tag(nltk.word_tokenize(word))[0][1])\n","                    pos_of_word = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n","                    if (pos_of_word == pospriors[word_pointer]):\n","                      word_pointer = word_pointer + 1\n","                      end_of_last_word = i + 1 \n","                    else: \n","                      print(\"Failed\")\n","                      del observed_seq[end_of_last_word:len(observed_seq)]\n","                      i = end_of_last_word - 1\n","                      if (pospriors[word_pointer][1] == \",\"):\n","                        observed_seq.append(\", \")\n","                        word_pointer = word_pointer + 1\n","                  else: \n","                    i = i - 1\n","                else:\n","                  observed_seq.append(int2char[ch_id])\n","                i = i + 1\n","\n","        return ''.join(observed_seq)\n","      \n","      \n","np.random.seed(123)\n","rnn = CharRNN(len(tweetChars), sampling=True)\n","print(rnn.sample(ckpt_dir='/content/drive/My Drive/model-100/', output_length=70,pospriors = ['DT', 'NN', 'VBZ', 'JJ', 'CC', 'RB', 'DT', 'NN', 'IN', 'NN']))\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","DT\n","NNS\n","VBP\n","DT\n","NN\n","NN\n","IN\n","DT\n","NN\n",",\n","CC\n","DT\n","NN\n","VBZ\n","VBN\n","IN\n","NNS\n","<< lstm_outputs >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 250), dtype=float32)\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/model-100/language_modeling.ckpt\n","['a', ' ', 't', 'h', 'e']\n","Word generated:  a the\n","Looking for:  DT\n","Word found:  DT\n","['p', 'o', 'i', 'n', 't']\n","Word generated:  point\n","Looking for:  NN\n","Word found:  NN\n","['o', 'f']\n","Word generated:  of\n","Looking for:  VBZ\n","Word found:  IN\n","Failed\n","['t', 'e', 'r', 'r', 'o', 'r', 'i', 's', 'm']\n","Word generated:  terrorism\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","[]\n","Word generated:  \n","[' ', 't', 'h']\n","Word generated:   th\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['s', 'a', 'm', 'e']\n","Word generated:  same\n","Looking for:  VBZ\n","Word found:  JJ\n","Failed\n","['t', 'h', 'i', 'n', 'g']\n","Word generated:  thing\n","Looking for:  VBZ\n","Word found:  NN\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  VBZ\n","Word found:  VBZ\n","['a', 'l', 'w', 'a', 'y', 's']\n","Word generated:  always\n","Looking for:  JJ\n","Word found:  RB\n","Failed\n","['a', 'l', 'l']\n","Word generated:  all\n","Looking for:  JJ\n","Word found:  DT\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  JJ\n","Word found:  DT\n","Failed\n","['m', 'i', 'n', 'd']\n","Word generated:  mind\n","Looking for:  JJ\n","Word found:  NN\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  JJ\n","Word found:  TO\n","Failed\n","['s', 't', 'a', 'y']\n","Word generated:  stay\n","Looking for:  JJ\n","Word found:  NN\n","Failed\n","['i', 't']\n","Word generated:  it\n","Looking for:  JJ\n","Word found:  PRP\n","Failed\n","['w', 'i', 't', 'h']\n","Word generated:  with\n","Looking for:  JJ\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  JJ\n","Word found:  DT\n","Failed\n","['p', 'o', 'i', 'n', 't']\n","Word generated:  point\n","Looking for:  JJ\n","Word found:  NN\n","Failed\n","['o', 'f']\n","Word generated:  of\n","Looking for:  JJ\n","Word found:  IN\n","Failed\n","['c', 'o', 'n', 's', 'c', 'i', 'o', 'u', 's', 'n', 'e', 's', 's']\n","Word generated:  consciousness\n","Looking for:  JJ\n","Word found:  NN\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  JJ\n","Word found:  VBZ\n","Failed\n","['a']\n","Word generated:  a\n","Looking for:  JJ\n","Word found:  DT\n","Failed\n","['p', 'r', 'o', 'c', 'e', 's', 's']\n","Word generated:  process\n","Looking for:  JJ\n","Word found:  NN\n","Failed\n","['i', 'f']\n","Word generated:  if\n","Looking for:  JJ\n","Word found:  IN\n","Failed\n","['y', 'o', 'u']\n","Word generated:  you\n","Looking for:  JJ\n","Word found:  PRP\n","Failed\n","['a', 'r', 'e']\n","Word generated:  are\n","Looking for:  JJ\n","Word found:  VBP\n","Failed\n","['s', 't', 'e', 'p', 's']\n","Word generated:  steps\n","Looking for:  JJ\n","Word found:  NNS\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  JJ\n","Word found:  DT\n","Failed\n","['w', 'h', 'o', 'l', 'e']\n","Word generated:  whole\n","Looking for:  JJ\n","Word found:  JJ\n","['s', 't', 'e', 'p']\n","Word generated:  step\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['o', 'f']\n","Word generated:  of\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['s', 't', 'e', 'p']\n","Word generated:  step\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  CC\n","Word found:  DT\n","Failed\n","['w', 'o', 'r', 'l', 'd']\n","Word generated:  world\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  CC\n","Word found:  VBZ\n","Failed\n","['s', 'u', 'p', 'p', 'o', 'r', 't']\n","Word generated:  support\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['t', 'h', 'a', 't']\n","Word generated:  that\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['i', 't']\n","Word generated:  it\n","Looking for:  CC\n","Word found:  PRP\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  CC\n","Word found:  VBZ\n","Failed\n","['n', 'o', 'w']\n","Word generated:  now\n","Looking for:  CC\n","Word found:  RB\n","Failed\n","['t', 'h', 'e', 'y']\n","Word generated:  they\n","Looking for:  CC\n","Word found:  PRP\n","Failed\n","['d', 'o', 'n', 't']\n","Word generated:  dont\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['h', 'a', 'v', 'e']\n","Word generated:  have\n","Looking for:  CC\n","Word found:  VB\n","Failed\n","['t', 'o']\n","Word generated:  to\n","Looking for:  CC\n","Word found:  TO\n","Failed\n","['b', 'e']\n","Word generated:  be\n","Looking for:  CC\n","Word found:  VB\n","Failed\n","['a']\n","Word generated:  a\n","Looking for:  CC\n","Word found:  DT\n","Failed\n","['m', 'a', 's', 's', 'a']\n","Word generated:  massa\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['c', 'h', 'r', 'i', 'c', 't', 'i', 'e']\n","Word generated:  chrictie\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['a', 's']\n","Word generated:  as\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['t', 'r', 'a', 'd', 'i', 't', 'i', 'o', 'n', 'a', 'l']\n","Word generated:  traditional\n","Looking for:  CC\n","Word found:  JJ\n","Failed\n","['t', 'e', 'n']\n","Word generated:  ten\n","Looking for:  CC\n","Word found:  NNS\n","Failed\n","['m', 'i', 'g', 'r', 'a', 'n', 't', 's']\n","Word generated:  migrants\n","Looking for:  CC\n","Word found:  NNS\n","Failed\n","['t', 'h', 'a', 't']\n","Word generated:  that\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['s', 'h', 'e', 'r']\n","Word generated:  sher\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['s', 'a', 't', 'e', 's']\n","Word generated:  sates\n","Looking for:  CC\n","Word found:  NNS\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  CC\n","Word found:  IN\n","Failed\n","['e', 'v', 'e', 'n']\n","Word generated:  even\n","Looking for:  CC\n","Word found:  RB\n","Failed\n","['s', 'o']\n","Word generated:  so\n","Looking for:  CC\n","Word found:  RB\n","Failed\n","['i', 't', 's']\n","Word generated:  its\n","Looking for:  CC\n","Word found:  PRP$\n","Failed\n","['i']\n","Word generated:  i\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['d', 'i', 's', 'a', 'p', 'p', 'r', 'o', 'm', 'a', 't', 'i', 'o', 'n']\n","Word generated:  disappromation\n","Looking for:  CC\n","Word found:  NN\n","Failed\n","['a', 'n', 'd']\n","Word generated:  and\n","Looking for:  CC\n","Word found:  CC\n","['s', 'o', 'l', 'e', 't']\n","Word generated:  solet\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['i', 'n', 't', 'o']\n","Word generated:  into\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['i', 'n', 'd', 'i', 'a']\n","Word generated:  india\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  RB\n","Word found:  DT\n","Failed\n","['m', 'o', 's', 't']\n","Word generated:  most\n","Looking for:  RB\n","Word found:  JJS\n","Failed\n","['c', 'h', 'a', 'o', 's']\n","Word generated:  chaos\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['o', 'f']\n","Word generated:  of\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['a']\n","Word generated:  a\n","Looking for:  RB\n","Word found:  DT\n","Failed\n","['c', 'r', 'e', 'a', 't', 'i', 'v', 'e']\n","Word generated:  creative\n","Looking for:  RB\n","Word found:  JJ\n","Failed\n","['p', 'e', 'r', 's', 'o', 'n']\n","Word generated:  person\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['a', 'l', 'o', 'n', 'g']\n","Word generated:  along\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['i', 'n']\n","Word generated:  in\n","Looking for:  RB\n","Word found:  IN\n","Failed\n","['m', 'e']\n","Word generated:  me\n","Looking for:  RB\n","Word found:  PRP\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  RB\n","Word found:  DT\n","Failed\n","['s', 'a', 'm', 'e']\n","Word generated:  same\n","Looking for:  RB\n","Word found:  JJ\n","Failed\n","['t', 'h', 'i', 'n', 'g']\n","Word generated:  thing\n","Looking for:  RB\n","Word found:  NN\n","Failed\n","['i', 's']\n","Word generated:  is\n","Looking for:  RB\n","Word found:  VBZ\n","Failed\n","['i', 'n', 's', 't', 'e', 'a', 'd']\n","Word generated:  instead\n","Looking for:  RB\n","Word found:  RB\n","['y', 'o', 'u']\n","Word generated:  you\n","Looking for:  DT\n","Word found:  PRP\n","Failed\n","['a', 'r', 'e']\n","Word generated:  are\n","Looking for:  DT\n","Word found:  VBP\n","Failed\n","['a', 'l', 'l']\n","Word generated:  all\n","Looking for:  DT\n","Word found:  DT\n","['o', 'f']\n","Word generated:  of\n","Looking for:  NN\n","Word found:  IN\n","Failed\n","['u', 's']\n","Word generated:  us\n","Looking for:  NN\n","Word found:  PRP\n","Failed\n","['s', 'e', 't', 't', 'l', 'e']\n","Word generated:  settle\n","Looking for:  NN\n","Word found:  VB\n","Failed\n","['a', 's']\n","Word generated:  as\n","Looking for:  NN\n","Word found:  IN\n","Failed\n","['t', 'h', 'e']\n","Word generated:  the\n","Looking for:  NN\n","Word found:  DT\n","Failed\n","['m', 'e', 'a', 'n', 'i', 'n', 'g']\n","Word generated:  meaning\n","Looking for:  NN\n","Word found:  NN\n","['o', 'f']\n","Word generated:  of\n","Looking for:  IN\n","Word found:  IN\n","['e', 'm', 'e', 'r', 'g', 'e', 'n', 'c', 'e']\n","Word generated:  emergence\n","Looking for:  NN\n","Word found:  NN\n","a the point is whole and instead\n","all meaning of emergence \n"],"name":"stdout"}]}]}